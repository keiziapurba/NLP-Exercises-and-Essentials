{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd5ffff",
   "metadata": {},
   "source": [
    "# Text Representation with Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46284b",
   "metadata": {},
   "source": [
    "## Exploring Traditional Statistical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c52b9",
   "metadata": {},
   "source": [
    "Feature Engineering is often known as the secret sauce to creating superior and better performing machine learning models. The importance of feature engineering is even more important for unstructured, textual data because we need to convert free flowing text into some numeric representations which can then be understood by machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8466b",
   "metadata": {},
   "source": [
    "Here we will explore the following feature engineering techniques:\n",
    "\n",
    "- Bag of Words Model (TF)\n",
    "- Bag of N-grams Model\n",
    "- TF-IDF Model\n",
    "- Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a23636",
   "metadata": {},
   "source": [
    "# Prepare a Sample Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba585b1",
   "metadata": {},
   "source": [
    "Let’s now take a sample corpus of documents on which we will run most of our analyses in this article. A corpus is typically a collection of text documents usually belonging to one or more subjects or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9992f9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document Category\n",
       "0                                      The sky is blue and beautiful.  weather\n",
       "1                                   Love this blue and beautiful sky!  weather\n",
       "2                        The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
       "4                         I love green eggs, ham, sausages and bacon!     food\n",
       "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
       "6            The sky is very blue and the sky is very beautiful today  weather\n",
       "7                         The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c473c5e",
   "metadata": {},
   "source": [
    "You can see that we have taken a few sample text documents belonging to different categories for our toy corpus. Before we talk about feature engineering, as always, we need to do some data pre-processing or wrangling to remove unnecessary characters, symbols and tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42f2e4",
   "metadata": {},
   "source": [
    "# Simple Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67d4a5",
   "metadata": {},
   "source": [
    "Since the focus of this unit is on feature engineering, we will build a simple text pre-processor which focuses on removing special characters, extra whitespaces, digits, stopwords and lower casing the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9b1227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/keiziapurba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/keiziapurba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f547b33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b90182",
   "metadata": {},
   "source": [
    "# Bag of Words Model - TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224559c1",
   "metadata": {},
   "source": [
    "This is perhaps the most simple vector space representational model for unstructured text. A vector space model is simply a mathematical model to represent unstructured text (or any other data) as numeric vectors, such that each dimension of the vector is a specific feature\\attribute. The bag of words model represents each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0) or even weighted values. The model’s name is such because each document is represented literally as a ‘bag’ of its own words, disregarding word orders, sequences and grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae47ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029817c",
   "metadata": {},
   "source": [
    "Thus you can see that our documents have been converted into numeric vectors such that each document is represented by one vector (row) in the above feature matrix. The following code will help represent this in a more easy to understand format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27cbd617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n",
       "0      0      0          1     1          0      0    0     0    0      0   \n",
       "1      0      0          1     1          0      0    0     0    0      0   \n",
       "2      0      0          0     0          0      1    1     0    1      0   \n",
       "3      1      1          0     0          1      0    0     1    0      0   \n",
       "4      1      0          0     0          0      0    0     1    0      1   \n",
       "5      0      0          0     1          0      1    1     0    1      0   \n",
       "6      0      0          1     1          0      0    0     0    0      0   \n",
       "7      0      0          0     0          0      1    1     0    1      0   \n",
       "\n",
       "   ham  jumps  kings  lazy  love  quick  sausages  sky  toast  today  \n",
       "0    0      0      0     0     0      0         0    1      0      0  \n",
       "1    0      0      0     0     1      0         0    1      0      0  \n",
       "2    0      1      0     1     0      1         0    0      0      0  \n",
       "3    1      0      1     0     0      0         1    0      1      0  \n",
       "4    1      0      0     0     1      0         1    0      0      0  \n",
       "5    0      0      0     1     0      1         0    0      0      0  \n",
       "6    0      0      0     0     0      0         0    2      0      1  \n",
       "7    0      0      0     1     0      1         0    0      0      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ae84c",
   "metadata": {},
   "source": [
    "This should make things more clearer! You can clearly see that each column or dimension in the feature vectors represents a word from the corpus and each row represents one of our documents. The value in any cell, represents the number of times that word (represented by column) occurs in the specific document (represented by row). Hence if a corpus of documents consists of N unique words across all the documents, we would have an N-dimensional vector for each of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5939021",
   "metadata": {},
   "source": [
    "# Bag of N-Grams Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b1cd0",
   "metadata": {},
   "source": [
    "A word is just a single token, often known as a unigram or 1-gram. We already know that the Bag of Words model doesn’t consider order of words. But what if we also wanted to take into account phrases or collection of words which occur in a sequence? N-grams help us achieve that. An N-gram is basically a collection of word tokens from a text document such that these tokens are contiguous and occur in a sequence. Bi-grams indicate n-grams of order 2 (two words), Tri-grams indicate n-grams of order 3 (three words), and so on. The Bag of N-Grams model is hence just an extension of the Bag of Words model so we can also leverage N-gram based features. The following example depicts bi-gram based features in each document feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c54605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon eggs</th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>beautiful today</th>\n",
       "      <th>blue beautiful</th>\n",
       "      <th>blue dog</th>\n",
       "      <th>blue sky</th>\n",
       "      <th>breakfast sausages</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog lazy</th>\n",
       "      <th>eggs ham</th>\n",
       "      <th>...</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>love blue</th>\n",
       "      <th>love green</th>\n",
       "      <th>quick blue</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>sausages bacon</th>\n",
       "      <th>sausages ham</th>\n",
       "      <th>sky beautiful</th>\n",
       "      <th>sky blue</th>\n",
       "      <th>toast beans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon eggs  beautiful sky  beautiful today  blue beautiful  blue dog  \\\n",
       "0           0              0                0               1         0   \n",
       "1           0              1                0               1         0   \n",
       "2           0              0                0               0         0   \n",
       "3           1              0                0               0         0   \n",
       "4           0              0                0               0         0   \n",
       "5           0              0                0               0         1   \n",
       "6           0              0                1               0         0   \n",
       "7           0              0                0               0         0   \n",
       "\n",
       "   blue sky  breakfast sausages  brown fox  dog lazy  eggs ham  ...  lazy dog  \\\n",
       "0         0                   0          0         0         0  ...         0   \n",
       "1         0                   0          0         0         0  ...         0   \n",
       "2         0                   0          1         0         0  ...         1   \n",
       "3         0                   1          0         0         0  ...         0   \n",
       "4         0                   0          0         0         1  ...         0   \n",
       "5         0                   0          1         1         0  ...         0   \n",
       "6         1                   0          0         0         0  ...         0   \n",
       "7         0                   0          1         1         0  ...         0   \n",
       "\n",
       "   love blue  love green  quick blue  quick brown  sausages bacon  \\\n",
       "0          0           0           0            0               0   \n",
       "1          1           0           0            0               0   \n",
       "2          0           0           0            1               0   \n",
       "3          0           0           0            0               0   \n",
       "4          0           1           0            0               1   \n",
       "5          0           0           1            0               0   \n",
       "6          0           0           0            0               0   \n",
       "7          0           0           0            0               0   \n",
       "\n",
       "   sausages ham  sky beautiful  sky blue  toast beans  \n",
       "0             0              0         1            0  \n",
       "1             0              0         0            0  \n",
       "2             0              0         0            0  \n",
       "3             1              0         0            1  \n",
       "4             0              0         0            0  \n",
       "5             0              0         0            0  \n",
       "6             0              1         1            0  \n",
       "7             0              0         0            0  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2,2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "pd.DataFrame(bv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2af5a",
   "metadata": {},
   "source": [
    "This gives us feature vectors for our documents, where each feature consists of a bi-gram representing a sequence of two words and values represent how many times the bi-gram was present for our documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980b285",
   "metadata": {},
   "source": [
    "# TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f03f77",
   "metadata": {},
   "source": [
    "There are some potential problems which might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms which occur frequently across all documents and these may tend to overshadow other terms in the feature set. The TF-IDF model tries to combat this issue by using a scaling or normalizing factor in its computation. TF-IDF stands for Term Frequency-Inverse Document Frequency, which uses a combination of two metrics in its computation, namely: term frequency (tf) and inverse document frequency (idf). This technique was developed for ranking results for queries in search engines and now it is an indispensable model in the world of information retrieval and NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f5d25",
   "metadata": {},
   "source": [
    "Mathematically, we can define TF-IDF as tfidf = tf x idf, which can be expanded further to be represented as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b894a3d",
   "metadata": {},
   "source": [
    "Here, tfidf(w, D) is the TF-IDF score for word w in document D.\n",
    "\n",
    "- The term tf(w, D) represents the term frequency of the word w in document D, which can be obtained from the Bag of Words model.\n",
    "- The term idf(w, D) is the inverse document frequency for the term w, which can be computed as the log transform of the total number of documents in the corpus C divided by the document frequency of the word w, which is basically the frequency of documents in the corpus where the word w occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff760fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n",
       "0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n",
       "3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n",
       "4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n",
       "5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n",
       "6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n",
       "\n",
       "    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n",
       "1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n",
       "2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n",
       "3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n",
       "4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n",
       "5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n",
       "6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n",
       "7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7b073",
   "metadata": {},
   "source": [
    "The TF-IDF based feature vectors for each of our text documents show scaled and normalized values as compared to the raw Bag of Words model values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b172743",
   "metadata": {},
   "source": [
    "# Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da186f",
   "metadata": {},
   "source": [
    "Document similarity is the process of using a distance or similarity based metric that can be used to identify how similar a text document is with any other document(s) based on features extracted from the documents like bag of words or tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3aa226",
   "metadata": {},
   "source": [
    "Thus you can see that we can build on top of the tf-idf based features we engineered in the previous section and use them to generate new features which can be useful in domains like search engines, document clustering and information retrieval by leveraging these similarity based features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36a922",
   "metadata": {},
   "source": [
    "Pairwise document similarity in a corpus involves computing document similarity for each pair of documents in a corpus. Thus if you have C documents in a corpus, you would end up with a C x C matrix such that each row and column represents the similarity score for a pair of documents, which represent the indices at the row and column, respectively. There are several similarity and distance metrics that are used to compute document similarity. These include cosine distance/similarity, euclidean distance, manhattan distance, BM25 similarity, jaccard distance and so on. In our analysis, we will be using perhaps the most popular and widely used similarity metric, cosine similarity and compare pairwise document similarity based on their TF-IDF feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54a3195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192353</td>\n",
       "      <td>0.817246</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.820599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225489</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.670631</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.192353</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.791821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115488</td>\n",
       "      <td>0.930989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.817246</td>\n",
       "      <td>0.670631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.000000  0.820599  0.000000  0.000000  0.000000  0.192353  0.817246   \n",
       "1  0.820599  1.000000  0.000000  0.000000  0.225489  0.157845  0.670631   \n",
       "2  0.000000  0.000000  1.000000  0.000000  0.000000  0.791821  0.000000   \n",
       "3  0.000000  0.000000  0.000000  1.000000  0.506866  0.000000  0.000000   \n",
       "4  0.000000  0.225489  0.000000  0.506866  1.000000  0.000000  0.000000   \n",
       "5  0.192353  0.157845  0.791821  0.000000  0.000000  1.000000  0.115488   \n",
       "6  0.817246  0.670631  0.000000  0.000000  0.000000  0.115488  1.000000   \n",
       "7  0.000000  0.000000  0.850516  0.000000  0.000000  0.930989  0.000000   \n",
       "\n",
       "          7  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.850516  \n",
       "3  0.000000  \n",
       "4  0.000000  \n",
       "5  0.930989  \n",
       "6  0.000000  \n",
       "7  1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ea359",
   "metadata": {},
   "source": [
    "Cosine similarity basically gives us a metric representing the cosine of the angle between the feature vector representations of two text documents. Lower the angle between the documents, the closer and more similar they are as depicted in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f00353",
   "metadata": {},
   "source": [
    "Looking closely at the similarity matrix clearly tells us that documents (0, 1 and 6), (2, 5 and 7) are very similar to one another and documents 3 and 4 are slightly similar to each other but the magnitude is not very strong, however still stronger than the other documents. This must indicate these similar documents have some similar features. This is a perfect example of grouping or clustering that can be solved by unsupervised learning especially when you are dealing with huge corpora of millions of text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbf360",
   "metadata": {},
   "source": [
    "# Clustering using Document Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12855b",
   "metadata": {},
   "source": [
    "We will use a very popular partition based clustering method, K-means clustering to cluster or group these documents based on their similarity based feature representations. In K-means clustering, we have an input parameter k, which specifies the number of clusters it will output using the document features. This clustering method is a centroid based clustering method, where it tries to cluster these documents into clusters of equal variance. It tries to create these clusters by minimizing the within-cluster sum of squares measure, also known as inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f918395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document  \\\n",
       "0                                      The sky is blue and beautiful.   \n",
       "1                                   Love this blue and beautiful sky!   \n",
       "2                        The quick brown fox jumps over the lazy dog.   \n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans   \n",
       "4                         I love green eggs, ham, sausages and bacon!   \n",
       "5                    The brown fox is quick and the blue dog is lazy!   \n",
       "6            The sky is very blue and the sky is very beautiful today   \n",
       "7                         The dog is lazy but the brown fox is quick!   \n",
       "\n",
       "  Category  ClusterLabel  \n",
       "0  weather             2  \n",
       "1  weather             2  \n",
       "2  animals             1  \n",
       "3     food             0  \n",
       "4     food             0  \n",
       "5  animals             1  \n",
       "6  weather             2  \n",
       "7  animals             1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "km.fit_transform(similarity_matrix)\n",
    "cluster_labels = km.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4782c7",
   "metadata": {},
   "source": [
    "We can see from the above output that our documents were correctly assigned to the right clusters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab60b64",
   "metadata": {},
   "source": [
    "## Exploring Word Embeddings with New Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a442daf",
   "metadata": {},
   "source": [
    "Traditional (count-based) feature engineering strategies for textual data involve models belonging to a family of models popularly known as the Bag of Words model. This includes term frequencies, TF-IDF (term frequency-inverse document frequency), N-grams and so on. While they are effective methods for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose additional information like the semantics, structure, sequence and context around nearby words in each text document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef55559",
   "metadata": {},
   "source": [
    "This forms as enough motivation for us to explore more sophisticated models which can capture this information and give us features which are vector representation of words, popularly known as embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008197c4",
   "metadata": {},
   "source": [
    "Here we will explore the following feature engineering techniques:\n",
    "\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc3778",
   "metadata": {},
   "source": [
    "Predictive methods like Neural Network based language models try to predict words from its neighboring words looking at word sequences in the corpus and in the process it learns distributed representations giving us dense word embeddings. We will be focusing on these predictive methods in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f977cdb",
   "metadata": {},
   "source": [
    "# The Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ffe6b",
   "metadata": {},
   "source": [
    "This model was created by Google in 2013 and is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity. Essentially these are unsupervised models which can take in massive textual corpora, create a vocabulary of possible words and generate dense word embeddings for each word in the vector space representing that vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbee9f",
   "metadata": {},
   "source": [
    "Usually you can specify the size of the word embedding vectors and the total number of vectors are essentially the size of the vocabulary. This makes the dimensionality of this dense vector space much lower than the high-dimensional sparse vector space built using traditional Bag of Words models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d3a76",
   "metadata": {},
   "source": [
    "There are two different model architectures which can be leveraged by Word2Vec to create these word embedding representations. These include,\n",
    "\n",
    "- The Continuous Bag of Words (CBOW) Model\n",
    "- The Skip-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aae91b",
   "metadata": {},
   "source": [
    "## 1. The Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed765d65",
   "metadata": {},
   "source": [
    "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82ea72",
   "metadata": {},
   "source": [
    "Considering a simple sentence, “the quick brown fox jumps over the lazy dog”, this can be pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy) and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e07c84",
   "metadata": {},
   "source": [
    "Thus the model tries to predict the target_word based on the context_window words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88dd9b2",
   "metadata": {},
   "source": [
    "## 2. The Skip-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe986f0",
   "metadata": {},
   "source": [
    "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c31e5",
   "metadata": {},
   "source": [
    "Considering our simple sentence from earlier, “the quick brown fox jumps over the lazy dog”. If we used the CBOW model, we get pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy) and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e824db9",
   "metadata": {},
   "source": [
    "Now considering that the skip-gram model’s aim is to predict the context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word from its target word. Hence the task becomes to predict the context [quick, fox] given target word ‘brown’ or [the, brown] given target word ‘quick’ and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4544fe",
   "metadata": {},
   "source": [
    "Thus the model tries to predict the context_window words based on the target_word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc94a49",
   "metadata": {},
   "source": [
    "# Robust Word2Vec Model with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc911b8",
   "metadata": {},
   "source": [
    "The **gensim** framework, created by Radim Řehůřek consists of a robust, efficient and scalable implementation of the Word2Vec model. We will leverage the same on our sample toy corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d9187",
   "metadata": {},
   "source": [
    "- **size**: The word embedding dimensionality\n",
    "- **window**: The context window size\n",
    "- **min_count**: The minimum word count\n",
    "- **sample**: The downsample setting for frequent words\n",
    "- **sg**: Training model, 1 for skip-gram otherwise CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a434d",
   "metadata": {},
   "source": [
    "We will build a simple Word2Vec model on the corpus and visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4862db32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f85b56e6eb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, vector_size=feature_size, \n",
    "                              window=window_context, min_count = min_word_count,\n",
    "                              sg=sg, sample=sample, epochs=5000)\n",
    "w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459bc29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# visualize embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = w2v_model.wv.index_to_key\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a040f9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.63943213, -0.02467916,  1.502709  ,  0.61027026, -0.7216693 ,\n",
       "        -0.3160052 ,  0.965002  ,  1.0018466 , -0.23420393,  0.7054972 ,\n",
       "         0.5145384 ,  0.87736785, -0.7516157 , -0.47060892,  0.07128179],\n",
       "       dtype=float32),\n",
       " (15,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['sky'], w2v_model.wv['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62a8d404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>-0.639432</td>\n",
       "      <td>-0.024679</td>\n",
       "      <td>1.502709</td>\n",
       "      <td>0.610270</td>\n",
       "      <td>-0.721669</td>\n",
       "      <td>-0.316005</td>\n",
       "      <td>0.965002</td>\n",
       "      <td>1.001847</td>\n",
       "      <td>-0.234204</td>\n",
       "      <td>0.705497</td>\n",
       "      <td>0.514538</td>\n",
       "      <td>0.877368</td>\n",
       "      <td>-0.751616</td>\n",
       "      <td>-0.470609</td>\n",
       "      <td>0.071282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-0.602743</td>\n",
       "      <td>0.073659</td>\n",
       "      <td>0.724945</td>\n",
       "      <td>0.157942</td>\n",
       "      <td>-0.789465</td>\n",
       "      <td>0.215237</td>\n",
       "      <td>0.614432</td>\n",
       "      <td>0.679531</td>\n",
       "      <td>-0.048113</td>\n",
       "      <td>0.658686</td>\n",
       "      <td>0.095931</td>\n",
       "      <td>0.815805</td>\n",
       "      <td>-0.159200</td>\n",
       "      <td>-0.532422</td>\n",
       "      <td>-0.248695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-0.988989</td>\n",
       "      <td>0.319340</td>\n",
       "      <td>0.843687</td>\n",
       "      <td>-1.026609</td>\n",
       "      <td>-0.590684</td>\n",
       "      <td>0.173463</td>\n",
       "      <td>0.259514</td>\n",
       "      <td>-0.637999</td>\n",
       "      <td>0.504987</td>\n",
       "      <td>0.112339</td>\n",
       "      <td>-0.474345</td>\n",
       "      <td>1.303706</td>\n",
       "      <td>-0.684338</td>\n",
       "      <td>-1.083380</td>\n",
       "      <td>0.311884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.544188</td>\n",
       "      <td>-0.353933</td>\n",
       "      <td>1.410363</td>\n",
       "      <td>0.720159</td>\n",
       "      <td>-0.013280</td>\n",
       "      <td>-0.414286</td>\n",
       "      <td>0.896078</td>\n",
       "      <td>0.552768</td>\n",
       "      <td>-0.145792</td>\n",
       "      <td>1.038409</td>\n",
       "      <td>0.312691</td>\n",
       "      <td>0.896521</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.480671</td>\n",
       "      <td>0.334769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>-0.789416</td>\n",
       "      <td>0.323791</td>\n",
       "      <td>0.290438</td>\n",
       "      <td>-0.849889</td>\n",
       "      <td>-0.650918</td>\n",
       "      <td>0.278436</td>\n",
       "      <td>-0.141102</td>\n",
       "      <td>-0.187405</td>\n",
       "      <td>0.451341</td>\n",
       "      <td>0.482975</td>\n",
       "      <td>0.027598</td>\n",
       "      <td>1.579678</td>\n",
       "      <td>-0.351136</td>\n",
       "      <td>-1.142641</td>\n",
       "      <td>0.484352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>-0.712770</td>\n",
       "      <td>0.695081</td>\n",
       "      <td>0.521078</td>\n",
       "      <td>-0.705848</td>\n",
       "      <td>-1.008142</td>\n",
       "      <td>0.141744</td>\n",
       "      <td>0.080199</td>\n",
       "      <td>-0.064908</td>\n",
       "      <td>0.848459</td>\n",
       "      <td>-0.062610</td>\n",
       "      <td>-0.022713</td>\n",
       "      <td>1.501421</td>\n",
       "      <td>-0.514936</td>\n",
       "      <td>-0.974268</td>\n",
       "      <td>0.667429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>-0.896241</td>\n",
       "      <td>0.404933</td>\n",
       "      <td>0.753256</td>\n",
       "      <td>-0.695366</td>\n",
       "      <td>-0.435905</td>\n",
       "      <td>0.327127</td>\n",
       "      <td>-0.346009</td>\n",
       "      <td>-0.465849</td>\n",
       "      <td>0.637149</td>\n",
       "      <td>0.599076</td>\n",
       "      <td>-0.256180</td>\n",
       "      <td>1.590770</td>\n",
       "      <td>-0.236885</td>\n",
       "      <td>-0.898548</td>\n",
       "      <td>0.597680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>-0.949771</td>\n",
       "      <td>0.453110</td>\n",
       "      <td>0.776009</td>\n",
       "      <td>-0.624677</td>\n",
       "      <td>-0.637290</td>\n",
       "      <td>0.142310</td>\n",
       "      <td>-0.439198</td>\n",
       "      <td>-0.422219</td>\n",
       "      <td>0.756097</td>\n",
       "      <td>0.316867</td>\n",
       "      <td>0.205115</td>\n",
       "      <td>1.447336</td>\n",
       "      <td>-0.020938</td>\n",
       "      <td>-0.981581</td>\n",
       "      <td>0.459399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>0.198648</td>\n",
       "      <td>0.176316</td>\n",
       "      <td>1.074163</td>\n",
       "      <td>-0.299473</td>\n",
       "      <td>1.010429</td>\n",
       "      <td>0.748644</td>\n",
       "      <td>-0.166153</td>\n",
       "      <td>0.556175</td>\n",
       "      <td>-0.244341</td>\n",
       "      <td>0.325445</td>\n",
       "      <td>0.850126</td>\n",
       "      <td>-0.310548</td>\n",
       "      <td>-0.408820</td>\n",
       "      <td>-0.935457</td>\n",
       "      <td>0.673599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.297497</td>\n",
       "      <td>0.631223</td>\n",
       "      <td>1.108528</td>\n",
       "      <td>0.457518</td>\n",
       "      <td>1.037477</td>\n",
       "      <td>1.027660</td>\n",
       "      <td>-0.164433</td>\n",
       "      <td>0.289555</td>\n",
       "      <td>-0.268838</td>\n",
       "      <td>-0.167446</td>\n",
       "      <td>0.689474</td>\n",
       "      <td>0.043164</td>\n",
       "      <td>-0.524910</td>\n",
       "      <td>-0.758180</td>\n",
       "      <td>0.730475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.566377</td>\n",
       "      <td>0.017698</td>\n",
       "      <td>0.900351</td>\n",
       "      <td>0.066918</td>\n",
       "      <td>0.828408</td>\n",
       "      <td>1.132224</td>\n",
       "      <td>0.374976</td>\n",
       "      <td>0.695544</td>\n",
       "      <td>-0.307900</td>\n",
       "      <td>0.289053</td>\n",
       "      <td>0.316389</td>\n",
       "      <td>-0.160405</td>\n",
       "      <td>-0.618389</td>\n",
       "      <td>-1.018001</td>\n",
       "      <td>0.789529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.123760</td>\n",
       "      <td>0.780017</td>\n",
       "      <td>0.875009</td>\n",
       "      <td>0.150625</td>\n",
       "      <td>0.959082</td>\n",
       "      <td>0.564490</td>\n",
       "      <td>-0.189527</td>\n",
       "      <td>0.971184</td>\n",
       "      <td>-0.022017</td>\n",
       "      <td>-0.024021</td>\n",
       "      <td>0.362688</td>\n",
       "      <td>-0.340379</td>\n",
       "      <td>-0.629636</td>\n",
       "      <td>-1.026932</td>\n",
       "      <td>0.675302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>-0.394468</td>\n",
       "      <td>0.130106</td>\n",
       "      <td>0.939160</td>\n",
       "      <td>0.273528</td>\n",
       "      <td>0.538580</td>\n",
       "      <td>-0.035836</td>\n",
       "      <td>0.319965</td>\n",
       "      <td>0.581181</td>\n",
       "      <td>-0.265422</td>\n",
       "      <td>0.164804</td>\n",
       "      <td>0.601005</td>\n",
       "      <td>-0.203412</td>\n",
       "      <td>-0.309521</td>\n",
       "      <td>-0.523189</td>\n",
       "      <td>0.022821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>0.303945</td>\n",
       "      <td>0.732523</td>\n",
       "      <td>0.551461</td>\n",
       "      <td>-0.690161</td>\n",
       "      <td>0.928490</td>\n",
       "      <td>0.908792</td>\n",
       "      <td>0.100185</td>\n",
       "      <td>0.380256</td>\n",
       "      <td>-0.258060</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>-0.398592</td>\n",
       "      <td>-0.401772</td>\n",
       "      <td>-1.365810</td>\n",
       "      <td>1.102824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.645077</td>\n",
       "      <td>0.723860</td>\n",
       "      <td>-0.297817</td>\n",
       "      <td>0.955470</td>\n",
       "      <td>0.756911</td>\n",
       "      <td>-0.050575</td>\n",
       "      <td>0.879580</td>\n",
       "      <td>-0.352204</td>\n",
       "      <td>-0.542636</td>\n",
       "      <td>0.328233</td>\n",
       "      <td>-0.276544</td>\n",
       "      <td>-0.517881</td>\n",
       "      <td>-1.302085</td>\n",
       "      <td>1.190832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>-0.155346</td>\n",
       "      <td>0.398843</td>\n",
       "      <td>1.293184</td>\n",
       "      <td>-0.010472</td>\n",
       "      <td>0.653003</td>\n",
       "      <td>0.602013</td>\n",
       "      <td>0.073694</td>\n",
       "      <td>0.627029</td>\n",
       "      <td>-0.486296</td>\n",
       "      <td>-0.198832</td>\n",
       "      <td>0.330482</td>\n",
       "      <td>-0.330293</td>\n",
       "      <td>-0.212875</td>\n",
       "      <td>-0.631169</td>\n",
       "      <td>0.266215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>-0.778245</td>\n",
       "      <td>0.525708</td>\n",
       "      <td>0.420622</td>\n",
       "      <td>-0.839927</td>\n",
       "      <td>-0.625751</td>\n",
       "      <td>0.449235</td>\n",
       "      <td>-0.172059</td>\n",
       "      <td>-0.387118</td>\n",
       "      <td>0.585515</td>\n",
       "      <td>0.127919</td>\n",
       "      <td>-0.163383</td>\n",
       "      <td>1.282363</td>\n",
       "      <td>-0.343933</td>\n",
       "      <td>-1.110705</td>\n",
       "      <td>0.370962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>0.085310</td>\n",
       "      <td>0.376509</td>\n",
       "      <td>0.538755</td>\n",
       "      <td>-0.546762</td>\n",
       "      <td>0.863889</td>\n",
       "      <td>1.001641</td>\n",
       "      <td>-0.074441</td>\n",
       "      <td>0.972542</td>\n",
       "      <td>-0.316990</td>\n",
       "      <td>-0.437855</td>\n",
       "      <td>0.775675</td>\n",
       "      <td>-0.169660</td>\n",
       "      <td>-0.332431</td>\n",
       "      <td>-1.338063</td>\n",
       "      <td>1.218791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.228510</td>\n",
       "      <td>0.852703</td>\n",
       "      <td>0.759716</td>\n",
       "      <td>-0.359444</td>\n",
       "      <td>0.837258</td>\n",
       "      <td>1.151343</td>\n",
       "      <td>0.072233</td>\n",
       "      <td>0.645075</td>\n",
       "      <td>-0.267103</td>\n",
       "      <td>-0.284198</td>\n",
       "      <td>0.544412</td>\n",
       "      <td>-0.407798</td>\n",
       "      <td>-0.677250</td>\n",
       "      <td>-1.231492</td>\n",
       "      <td>1.111453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>-0.769255</td>\n",
       "      <td>-0.159452</td>\n",
       "      <td>1.009941</td>\n",
       "      <td>0.578341</td>\n",
       "      <td>-0.280634</td>\n",
       "      <td>-0.289808</td>\n",
       "      <td>0.947467</td>\n",
       "      <td>0.950760</td>\n",
       "      <td>-0.258595</td>\n",
       "      <td>0.948960</td>\n",
       "      <td>0.594117</td>\n",
       "      <td>0.836471</td>\n",
       "      <td>-0.475224</td>\n",
       "      <td>-0.504680</td>\n",
       "      <td>0.038407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "sky       -0.639432 -0.024679  1.502709  0.610270 -0.721669 -0.316005   \n",
       "blue      -0.602743  0.073659  0.724945  0.157942 -0.789465  0.215237   \n",
       "lazy      -0.988989  0.319340  0.843687 -1.026609 -0.590684  0.173463   \n",
       "beautiful -0.544188 -0.353933  1.410363  0.720159 -0.013280 -0.414286   \n",
       "quick     -0.789416  0.323791  0.290438 -0.849889 -0.650918  0.278436   \n",
       "brown     -0.712770  0.695081  0.521078 -0.705848 -1.008142  0.141744   \n",
       "fox       -0.896241  0.404933  0.753256 -0.695366 -0.435905  0.327127   \n",
       "dog       -0.949771  0.453110  0.776009 -0.624677 -0.637290  0.142310   \n",
       "sausages   0.198648  0.176316  1.074163 -0.299473  1.010429  0.748644   \n",
       "ham        0.297497  0.631223  1.108528  0.457518  1.037477  1.027660   \n",
       "bacon      0.566377  0.017698  0.900351  0.066918  0.828408  1.132224   \n",
       "eggs       0.123760  0.780017  0.875009  0.150625  0.959082  0.564490   \n",
       "love      -0.394468  0.130106  0.939160  0.273528  0.538580 -0.035836   \n",
       "breakfast  0.303945  0.732523  0.551461 -0.690161  0.928490  0.908792   \n",
       "kings      0.000598  0.645077  0.723860 -0.297817  0.955470  0.756911   \n",
       "green     -0.155346  0.398843  1.293184 -0.010472  0.653003  0.602013   \n",
       "jumps     -0.778245  0.525708  0.420622 -0.839927 -0.625751  0.449235   \n",
       "toast      0.085310  0.376509  0.538755 -0.546762  0.863889  1.001641   \n",
       "beans      0.228510  0.852703  0.759716 -0.359444  0.837258  1.151343   \n",
       "today     -0.769255 -0.159452  1.009941  0.578341 -0.280634 -0.289808   \n",
       "\n",
       "                 6         7         8         9         10        11  \\\n",
       "sky        0.965002  1.001847 -0.234204  0.705497  0.514538  0.877368   \n",
       "blue       0.614432  0.679531 -0.048113  0.658686  0.095931  0.815805   \n",
       "lazy       0.259514 -0.637999  0.504987  0.112339 -0.474345  1.303706   \n",
       "beautiful  0.896078  0.552768 -0.145792  1.038409  0.312691  0.896521   \n",
       "quick     -0.141102 -0.187405  0.451341  0.482975  0.027598  1.579678   \n",
       "brown      0.080199 -0.064908  0.848459 -0.062610 -0.022713  1.501421   \n",
       "fox       -0.346009 -0.465849  0.637149  0.599076 -0.256180  1.590770   \n",
       "dog       -0.439198 -0.422219  0.756097  0.316867  0.205115  1.447336   \n",
       "sausages  -0.166153  0.556175 -0.244341  0.325445  0.850126 -0.310548   \n",
       "ham       -0.164433  0.289555 -0.268838 -0.167446  0.689474  0.043164   \n",
       "bacon      0.374976  0.695544 -0.307900  0.289053  0.316389 -0.160405   \n",
       "eggs      -0.189527  0.971184 -0.022017 -0.024021  0.362688 -0.340379   \n",
       "love       0.319965  0.581181 -0.265422  0.164804  0.601005 -0.203412   \n",
       "breakfast  0.100185  0.380256 -0.258060  0.202899  0.881259 -0.398592   \n",
       "kings     -0.050575  0.879580 -0.352204 -0.542636  0.328233 -0.276544   \n",
       "green      0.073694  0.627029 -0.486296 -0.198832  0.330482 -0.330293   \n",
       "jumps     -0.172059 -0.387118  0.585515  0.127919 -0.163383  1.282363   \n",
       "toast     -0.074441  0.972542 -0.316990 -0.437855  0.775675 -0.169660   \n",
       "beans      0.072233  0.645075 -0.267103 -0.284198  0.544412 -0.407798   \n",
       "today      0.947467  0.950760 -0.258595  0.948960  0.594117  0.836471   \n",
       "\n",
       "                 12        13        14  \n",
       "sky       -0.751616 -0.470609  0.071282  \n",
       "blue      -0.159200 -0.532422 -0.248695  \n",
       "lazy      -0.684338 -1.083380  0.311884  \n",
       "beautiful -0.635649 -0.480671  0.334769  \n",
       "quick     -0.351136 -1.142641  0.484352  \n",
       "brown     -0.514936 -0.974268  0.667429  \n",
       "fox       -0.236885 -0.898548  0.597680  \n",
       "dog       -0.020938 -0.981581  0.459399  \n",
       "sausages  -0.408820 -0.935457  0.673599  \n",
       "ham       -0.524910 -0.758180  0.730475  \n",
       "bacon     -0.618389 -1.018001  0.789529  \n",
       "eggs      -0.629636 -1.026932  0.675302  \n",
       "love      -0.309521 -0.523189  0.022821  \n",
       "breakfast -0.401772 -1.365810  1.102824  \n",
       "kings     -0.517881 -1.302085  1.190832  \n",
       "green     -0.212875 -0.631169  0.266215  \n",
       "jumps     -0.343933 -1.110705  0.370962  \n",
       "toast     -0.332431 -1.338063  1.218791  \n",
       "beans     -0.677250 -1.231492  1.111453  \n",
       "today     -0.475224 -0.504680  0.038407  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df = pd.DataFrame(wvs, index=words)\n",
    "vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974b22b4",
   "metadata": {},
   "source": [
    "## Looking at term semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d923af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sky</th>\n",
       "      <th>blue</th>\n",
       "      <th>lazy</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>quick</th>\n",
       "      <th>brown</th>\n",
       "      <th>fox</th>\n",
       "      <th>dog</th>\n",
       "      <th>sausages</th>\n",
       "      <th>ham</th>\n",
       "      <th>bacon</th>\n",
       "      <th>eggs</th>\n",
       "      <th>love</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>kings</th>\n",
       "      <th>green</th>\n",
       "      <th>jumps</th>\n",
       "      <th>toast</th>\n",
       "      <th>beans</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884443</td>\n",
       "      <td>0.404548</td>\n",
       "      <td>0.928573</td>\n",
       "      <td>0.403114</td>\n",
       "      <td>0.446656</td>\n",
       "      <td>0.376840</td>\n",
       "      <td>0.388981</td>\n",
       "      <td>0.293115</td>\n",
       "      <td>0.277195</td>\n",
       "      <td>0.349490</td>\n",
       "      <td>0.310484</td>\n",
       "      <td>0.670830</td>\n",
       "      <td>0.134928</td>\n",
       "      <td>0.197749</td>\n",
       "      <td>0.421173</td>\n",
       "      <td>0.311042</td>\n",
       "      <td>0.172593</td>\n",
       "      <td>0.185005</td>\n",
       "      <td>0.962221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.884443</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.544603</td>\n",
       "      <td>0.769207</td>\n",
       "      <td>0.599867</td>\n",
       "      <td>0.584544</td>\n",
       "      <td>0.544614</td>\n",
       "      <td>0.545837</td>\n",
       "      <td>0.151053</td>\n",
       "      <td>0.112449</td>\n",
       "      <td>0.238992</td>\n",
       "      <td>0.161952</td>\n",
       "      <td>0.448391</td>\n",
       "      <td>0.067489</td>\n",
       "      <td>0.083450</td>\n",
       "      <td>0.277041</td>\n",
       "      <td>0.522448</td>\n",
       "      <td>0.100712</td>\n",
       "      <td>0.094574</td>\n",
       "      <td>0.871930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>0.404548</td>\n",
       "      <td>0.544603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.386370</td>\n",
       "      <td>0.906429</td>\n",
       "      <td>0.899321</td>\n",
       "      <td>0.914341</td>\n",
       "      <td>0.878318</td>\n",
       "      <td>0.123732</td>\n",
       "      <td>0.101477</td>\n",
       "      <td>0.115611</td>\n",
       "      <td>0.104955</td>\n",
       "      <td>0.098807</td>\n",
       "      <td>0.204776</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.139880</td>\n",
       "      <td>0.944066</td>\n",
       "      <td>0.173519</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.341223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>0.928573</td>\n",
       "      <td>0.769207</td>\n",
       "      <td>0.386370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376902</td>\n",
       "      <td>0.351269</td>\n",
       "      <td>0.404056</td>\n",
       "      <td>0.367527</td>\n",
       "      <td>0.358419</td>\n",
       "      <td>0.328147</td>\n",
       "      <td>0.413883</td>\n",
       "      <td>0.319332</td>\n",
       "      <td>0.682948</td>\n",
       "      <td>0.167741</td>\n",
       "      <td>0.195188</td>\n",
       "      <td>0.398091</td>\n",
       "      <td>0.262262</td>\n",
       "      <td>0.160075</td>\n",
       "      <td>0.171848</td>\n",
       "      <td>0.945873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>0.403114</td>\n",
       "      <td>0.599867</td>\n",
       "      <td>0.906429</td>\n",
       "      <td>0.376902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931961</td>\n",
       "      <td>0.954882</td>\n",
       "      <td>0.941377</td>\n",
       "      <td>0.170716</td>\n",
       "      <td>0.120554</td>\n",
       "      <td>0.137238</td>\n",
       "      <td>0.133597</td>\n",
       "      <td>0.058925</td>\n",
       "      <td>0.270315</td>\n",
       "      <td>0.215114</td>\n",
       "      <td>0.066787</td>\n",
       "      <td>0.968901</td>\n",
       "      <td>0.257749</td>\n",
       "      <td>0.216935</td>\n",
       "      <td>0.394572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>0.446656</td>\n",
       "      <td>0.584544</td>\n",
       "      <td>0.899321</td>\n",
       "      <td>0.351269</td>\n",
       "      <td>0.931961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897303</td>\n",
       "      <td>0.915842</td>\n",
       "      <td>0.099398</td>\n",
       "      <td>0.129835</td>\n",
       "      <td>0.096733</td>\n",
       "      <td>0.164160</td>\n",
       "      <td>0.052679</td>\n",
       "      <td>0.222446</td>\n",
       "      <td>0.239225</td>\n",
       "      <td>0.083757</td>\n",
       "      <td>0.947395</td>\n",
       "      <td>0.242752</td>\n",
       "      <td>0.239352</td>\n",
       "      <td>0.366445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.376840</td>\n",
       "      <td>0.544614</td>\n",
       "      <td>0.914341</td>\n",
       "      <td>0.404056</td>\n",
       "      <td>0.954882</td>\n",
       "      <td>0.897303</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965174</td>\n",
       "      <td>0.183718</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.129820</td>\n",
       "      <td>0.147126</td>\n",
       "      <td>0.068539</td>\n",
       "      <td>0.227387</td>\n",
       "      <td>0.190613</td>\n",
       "      <td>0.113491</td>\n",
       "      <td>0.952296</td>\n",
       "      <td>0.195343</td>\n",
       "      <td>0.193750</td>\n",
       "      <td>0.353357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.388981</td>\n",
       "      <td>0.545837</td>\n",
       "      <td>0.878318</td>\n",
       "      <td>0.367527</td>\n",
       "      <td>0.941377</td>\n",
       "      <td>0.915842</td>\n",
       "      <td>0.965174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174181</td>\n",
       "      <td>0.161865</td>\n",
       "      <td>0.056952</td>\n",
       "      <td>0.135042</td>\n",
       "      <td>0.108004</td>\n",
       "      <td>0.217166</td>\n",
       "      <td>0.177226</td>\n",
       "      <td>0.116675</td>\n",
       "      <td>0.945430</td>\n",
       "      <td>0.202507</td>\n",
       "      <td>0.174167</td>\n",
       "      <td>0.354797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>0.293115</td>\n",
       "      <td>0.151053</td>\n",
       "      <td>0.123732</td>\n",
       "      <td>0.358419</td>\n",
       "      <td>0.170716</td>\n",
       "      <td>0.099398</td>\n",
       "      <td>0.183718</td>\n",
       "      <td>0.174181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883753</td>\n",
       "      <td>0.899110</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.755130</td>\n",
       "      <td>0.912560</td>\n",
       "      <td>0.855671</td>\n",
       "      <td>0.867339</td>\n",
       "      <td>0.161442</td>\n",
       "      <td>0.880381</td>\n",
       "      <td>0.888253</td>\n",
       "      <td>0.315121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.277195</td>\n",
       "      <td>0.112449</td>\n",
       "      <td>0.101477</td>\n",
       "      <td>0.328147</td>\n",
       "      <td>0.120554</td>\n",
       "      <td>0.129835</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.161865</td>\n",
       "      <td>0.883753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866839</td>\n",
       "      <td>0.890548</td>\n",
       "      <td>0.682801</td>\n",
       "      <td>0.811592</td>\n",
       "      <td>0.843729</td>\n",
       "      <td>0.855218</td>\n",
       "      <td>0.158935</td>\n",
       "      <td>0.811822</td>\n",
       "      <td>0.882894</td>\n",
       "      <td>0.254957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.349490</td>\n",
       "      <td>0.238992</td>\n",
       "      <td>0.115611</td>\n",
       "      <td>0.413883</td>\n",
       "      <td>0.137238</td>\n",
       "      <td>0.096733</td>\n",
       "      <td>0.129820</td>\n",
       "      <td>0.056952</td>\n",
       "      <td>0.899110</td>\n",
       "      <td>0.866839</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853019</td>\n",
       "      <td>0.654689</td>\n",
       "      <td>0.846136</td>\n",
       "      <td>0.837929</td>\n",
       "      <td>0.806857</td>\n",
       "      <td>0.129091</td>\n",
       "      <td>0.841791</td>\n",
       "      <td>0.883412</td>\n",
       "      <td>0.353559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.310484</td>\n",
       "      <td>0.161952</td>\n",
       "      <td>0.104955</td>\n",
       "      <td>0.319332</td>\n",
       "      <td>0.133597</td>\n",
       "      <td>0.164160</td>\n",
       "      <td>0.147126</td>\n",
       "      <td>0.135042</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.890548</td>\n",
       "      <td>0.853019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742760</td>\n",
       "      <td>0.847655</td>\n",
       "      <td>0.924934</td>\n",
       "      <td>0.865969</td>\n",
       "      <td>0.162399</td>\n",
       "      <td>0.867630</td>\n",
       "      <td>0.919483</td>\n",
       "      <td>0.298116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.670830</td>\n",
       "      <td>0.448391</td>\n",
       "      <td>0.098807</td>\n",
       "      <td>0.682948</td>\n",
       "      <td>0.058925</td>\n",
       "      <td>0.052679</td>\n",
       "      <td>0.068539</td>\n",
       "      <td>0.108004</td>\n",
       "      <td>0.755130</td>\n",
       "      <td>0.682801</td>\n",
       "      <td>0.654689</td>\n",
       "      <td>0.742760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.573420</td>\n",
       "      <td>0.626091</td>\n",
       "      <td>0.843869</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.583890</td>\n",
       "      <td>0.598949</td>\n",
       "      <td>0.698342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>0.134928</td>\n",
       "      <td>0.067489</td>\n",
       "      <td>0.204776</td>\n",
       "      <td>0.167741</td>\n",
       "      <td>0.270315</td>\n",
       "      <td>0.222446</td>\n",
       "      <td>0.227387</td>\n",
       "      <td>0.217166</td>\n",
       "      <td>0.912560</td>\n",
       "      <td>0.811592</td>\n",
       "      <td>0.846136</td>\n",
       "      <td>0.847655</td>\n",
       "      <td>0.573420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899567</td>\n",
       "      <td>0.744333</td>\n",
       "      <td>0.282219</td>\n",
       "      <td>0.929046</td>\n",
       "      <td>0.950689</td>\n",
       "      <td>0.159579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.197749</td>\n",
       "      <td>0.083450</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.195188</td>\n",
       "      <td>0.215114</td>\n",
       "      <td>0.239225</td>\n",
       "      <td>0.190613</td>\n",
       "      <td>0.177226</td>\n",
       "      <td>0.855671</td>\n",
       "      <td>0.843729</td>\n",
       "      <td>0.837929</td>\n",
       "      <td>0.924934</td>\n",
       "      <td>0.626091</td>\n",
       "      <td>0.899567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838363</td>\n",
       "      <td>0.253953</td>\n",
       "      <td>0.966309</td>\n",
       "      <td>0.966283</td>\n",
       "      <td>0.181938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.421173</td>\n",
       "      <td>0.277041</td>\n",
       "      <td>0.139880</td>\n",
       "      <td>0.398091</td>\n",
       "      <td>0.066787</td>\n",
       "      <td>0.083757</td>\n",
       "      <td>0.113491</td>\n",
       "      <td>0.116675</td>\n",
       "      <td>0.867339</td>\n",
       "      <td>0.855218</td>\n",
       "      <td>0.806857</td>\n",
       "      <td>0.865969</td>\n",
       "      <td>0.843869</td>\n",
       "      <td>0.744333</td>\n",
       "      <td>0.838363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116068</td>\n",
       "      <td>0.786012</td>\n",
       "      <td>0.830884</td>\n",
       "      <td>0.382024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>0.311042</td>\n",
       "      <td>0.522448</td>\n",
       "      <td>0.944066</td>\n",
       "      <td>0.262262</td>\n",
       "      <td>0.968901</td>\n",
       "      <td>0.947395</td>\n",
       "      <td>0.952296</td>\n",
       "      <td>0.945430</td>\n",
       "      <td>0.161442</td>\n",
       "      <td>0.158935</td>\n",
       "      <td>0.129091</td>\n",
       "      <td>0.162399</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.282219</td>\n",
       "      <td>0.253953</td>\n",
       "      <td>0.116068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.267103</td>\n",
       "      <td>0.267155</td>\n",
       "      <td>0.264548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>0.172593</td>\n",
       "      <td>0.100712</td>\n",
       "      <td>0.173519</td>\n",
       "      <td>0.160075</td>\n",
       "      <td>0.257749</td>\n",
       "      <td>0.242752</td>\n",
       "      <td>0.195343</td>\n",
       "      <td>0.202507</td>\n",
       "      <td>0.880381</td>\n",
       "      <td>0.811822</td>\n",
       "      <td>0.841791</td>\n",
       "      <td>0.867630</td>\n",
       "      <td>0.583890</td>\n",
       "      <td>0.929046</td>\n",
       "      <td>0.966309</td>\n",
       "      <td>0.786012</td>\n",
       "      <td>0.267103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950919</td>\n",
       "      <td>0.182520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.185005</td>\n",
       "      <td>0.094574</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.171848</td>\n",
       "      <td>0.216935</td>\n",
       "      <td>0.239352</td>\n",
       "      <td>0.193750</td>\n",
       "      <td>0.174167</td>\n",
       "      <td>0.888253</td>\n",
       "      <td>0.882894</td>\n",
       "      <td>0.883412</td>\n",
       "      <td>0.919483</td>\n",
       "      <td>0.598949</td>\n",
       "      <td>0.950689</td>\n",
       "      <td>0.966283</td>\n",
       "      <td>0.830884</td>\n",
       "      <td>0.267155</td>\n",
       "      <td>0.950919</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.962221</td>\n",
       "      <td>0.871930</td>\n",
       "      <td>0.341223</td>\n",
       "      <td>0.945873</td>\n",
       "      <td>0.394572</td>\n",
       "      <td>0.366445</td>\n",
       "      <td>0.353357</td>\n",
       "      <td>0.354797</td>\n",
       "      <td>0.315121</td>\n",
       "      <td>0.254957</td>\n",
       "      <td>0.353559</td>\n",
       "      <td>0.298116</td>\n",
       "      <td>0.698342</td>\n",
       "      <td>0.159579</td>\n",
       "      <td>0.181938</td>\n",
       "      <td>0.382024</td>\n",
       "      <td>0.264548</td>\n",
       "      <td>0.182520</td>\n",
       "      <td>0.159671</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sky      blue      lazy  beautiful     quick     brown  \\\n",
       "sky        1.000000  0.884443  0.404548   0.928573  0.403114  0.446656   \n",
       "blue       0.884443  1.000000  0.544603   0.769207  0.599867  0.584544   \n",
       "lazy       0.404548  0.544603  1.000000   0.386370  0.906429  0.899321   \n",
       "beautiful  0.928573  0.769207  0.386370   1.000000  0.376902  0.351269   \n",
       "quick      0.403114  0.599867  0.906429   0.376902  1.000000  0.931961   \n",
       "brown      0.446656  0.584544  0.899321   0.351269  0.931961  1.000000   \n",
       "fox        0.376840  0.544614  0.914341   0.404056  0.954882  0.897303   \n",
       "dog        0.388981  0.545837  0.878318   0.367527  0.941377  0.915842   \n",
       "sausages   0.293115  0.151053  0.123732   0.358419  0.170716  0.099398   \n",
       "ham        0.277195  0.112449  0.101477   0.328147  0.120554  0.129835   \n",
       "bacon      0.349490  0.238992  0.115611   0.413883  0.137238  0.096733   \n",
       "eggs       0.310484  0.161952  0.104955   0.319332  0.133597  0.164160   \n",
       "love       0.670830  0.448391  0.098807   0.682948  0.058925  0.052679   \n",
       "breakfast  0.134928  0.067489  0.204776   0.167741  0.270315  0.222446   \n",
       "kings      0.197749  0.083450  0.200766   0.195188  0.215114  0.239225   \n",
       "green      0.421173  0.277041  0.139880   0.398091  0.066787  0.083757   \n",
       "jumps      0.311042  0.522448  0.944066   0.262262  0.968901  0.947395   \n",
       "toast      0.172593  0.100712  0.173519   0.160075  0.257749  0.242752   \n",
       "beans      0.185005  0.094574  0.202247   0.171848  0.216935  0.239352   \n",
       "today      0.962221  0.871930  0.341223   0.945873  0.394572  0.366445   \n",
       "\n",
       "                fox       dog  sausages       ham     bacon      eggs  \\\n",
       "sky        0.376840  0.388981  0.293115  0.277195  0.349490  0.310484   \n",
       "blue       0.544614  0.545837  0.151053  0.112449  0.238992  0.161952   \n",
       "lazy       0.914341  0.878318  0.123732  0.101477  0.115611  0.104955   \n",
       "beautiful  0.404056  0.367527  0.358419  0.328147  0.413883  0.319332   \n",
       "quick      0.954882  0.941377  0.170716  0.120554  0.137238  0.133597   \n",
       "brown      0.897303  0.915842  0.099398  0.129835  0.096733  0.164160   \n",
       "fox        1.000000  0.965174  0.183718  0.176364  0.129820  0.147126   \n",
       "dog        0.965174  1.000000  0.174181  0.161865  0.056952  0.135042   \n",
       "sausages   0.183718  0.174181  1.000000  0.883753  0.899110  0.887931   \n",
       "ham        0.176364  0.161865  0.883753  1.000000  0.866839  0.890548   \n",
       "bacon      0.129820  0.056952  0.899110  0.866839  1.000000  0.853019   \n",
       "eggs       0.147126  0.135042  0.887931  0.890548  0.853019  1.000000   \n",
       "love       0.068539  0.108004  0.755130  0.682801  0.654689  0.742760   \n",
       "breakfast  0.227387  0.217166  0.912560  0.811592  0.846136  0.847655   \n",
       "kings      0.190613  0.177226  0.855671  0.843729  0.837929  0.924934   \n",
       "green      0.113491  0.116675  0.867339  0.855218  0.806857  0.865969   \n",
       "jumps      0.952296  0.945430  0.161442  0.158935  0.129091  0.162399   \n",
       "toast      0.195343  0.202507  0.880381  0.811822  0.841791  0.867630   \n",
       "beans      0.193750  0.174167  0.888253  0.882894  0.883412  0.919483   \n",
       "today      0.353357  0.354797  0.315121  0.254957  0.353559  0.298116   \n",
       "\n",
       "               love  breakfast     kings     green     jumps     toast  \\\n",
       "sky        0.670830   0.134928  0.197749  0.421173  0.311042  0.172593   \n",
       "blue       0.448391   0.067489  0.083450  0.277041  0.522448  0.100712   \n",
       "lazy       0.098807   0.204776  0.200766  0.139880  0.944066  0.173519   \n",
       "beautiful  0.682948   0.167741  0.195188  0.398091  0.262262  0.160075   \n",
       "quick      0.058925   0.270315  0.215114  0.066787  0.968901  0.257749   \n",
       "brown      0.052679   0.222446  0.239225  0.083757  0.947395  0.242752   \n",
       "fox        0.068539   0.227387  0.190613  0.113491  0.952296  0.195343   \n",
       "dog        0.108004   0.217166  0.177226  0.116675  0.945430  0.202507   \n",
       "sausages   0.755130   0.912560  0.855671  0.867339  0.161442  0.880381   \n",
       "ham        0.682801   0.811592  0.843729  0.855218  0.158935  0.811822   \n",
       "bacon      0.654689   0.846136  0.837929  0.806857  0.129091  0.841791   \n",
       "eggs       0.742760   0.847655  0.924934  0.865969  0.162399  0.867630   \n",
       "love       1.000000   0.573420  0.626091  0.843869  0.027090  0.583890   \n",
       "breakfast  0.573420   1.000000  0.899567  0.744333  0.282219  0.929046   \n",
       "kings      0.626091   0.899567  1.000000  0.838363  0.253953  0.966309   \n",
       "green      0.843869   0.744333  0.838363  1.000000  0.116068  0.786012   \n",
       "jumps      0.027090   0.282219  0.253953  0.116068  1.000000  0.267103   \n",
       "toast      0.583890   0.929046  0.966309  0.786012  0.267103  1.000000   \n",
       "beans      0.598949   0.950689  0.966283  0.830884  0.267155  0.950919   \n",
       "today      0.698342   0.159579  0.181938  0.382024  0.264548  0.182520   \n",
       "\n",
       "              beans     today  \n",
       "sky        0.185005  0.962221  \n",
       "blue       0.094574  0.871930  \n",
       "lazy       0.202247  0.341223  \n",
       "beautiful  0.171848  0.945873  \n",
       "quick      0.216935  0.394572  \n",
       "brown      0.239352  0.366445  \n",
       "fox        0.193750  0.353357  \n",
       "dog        0.174167  0.354797  \n",
       "sausages   0.888253  0.315121  \n",
       "ham        0.882894  0.254957  \n",
       "bacon      0.883412  0.353559  \n",
       "eggs       0.919483  0.298116  \n",
       "love       0.598949  0.698342  \n",
       "breakfast  0.950689  0.159579  \n",
       "kings      0.966283  0.181938  \n",
       "green      0.830884  0.382024  \n",
       "jumps      0.267155  0.264548  \n",
       "toast      0.950919  0.182520  \n",
       "beans      1.000000  0.159671  \n",
       "today      0.159671  1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(vec_df.values)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de49471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sky           [today, beautiful, blue]\n",
       "blue           [sky, today, beautiful]\n",
       "lazy               [jumps, fox, quick]\n",
       "beautiful           [today, sky, blue]\n",
       "quick                [jumps, fox, dog]\n",
       "brown              [jumps, quick, dog]\n",
       "fox                [dog, quick, jumps]\n",
       "dog                [fox, jumps, quick]\n",
       "sausages     [breakfast, bacon, beans]\n",
       "ham            [eggs, sausages, beans]\n",
       "bacon           [sausages, beans, ham]\n",
       "eggs               [kings, beans, ham]\n",
       "love           [green, sausages, eggs]\n",
       "breakfast     [beans, toast, sausages]\n",
       "kings             [toast, beans, eggs]\n",
       "green            [sausages, eggs, ham]\n",
       "jumps              [quick, fox, brown]\n",
       "toast        [kings, beans, breakfast]\n",
       "beans        [kings, toast, breakfast]\n",
       "today           [sky, beautiful, blue]\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(words)\n",
    "similarity_df.apply(lambda row: feature_names[np.argsort(-row.values)[1:4]], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22eafcf",
   "metadata": {},
   "source": [
    "# The GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0fd93",
   "metadata": {},
   "source": [
    "The GloVe model stands for Global Vectors which is an unsupervised learning model which can be used to obtain dense word vectors similar to Word2Vec. However the technique is different and training is performed on an aggregated global word-word co-occurrence matrix, giving us a vector space with meaningful sub-structures. This method was invented in Stanford by Pennington et al. and I recommend you to read the original paper on GloVe, ‘GloVe: Global Vectors for Word Representation’ by Pennington et al. which is an excellent read to get some perspective on how this model works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3df5e6",
   "metadata": {},
   "source": [
    "The basic methodology of the GloVe model is to first create a huge word-context co-occurence matrix consisting of (word, context) pairs such that each element in this matrix represents how often a word occurs with the context (which can be a sequence of words). The idea then is to apply matrix factorization to approximate this matrix as depicted in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d5be6",
   "metadata": {},
   "source": [
    "Considering the **Word-Context (WC)** matrix, **Word-Feature (WF)** matrix and **Feature-Context (FC)** matrix, we try to factorize **WC = WF x FC**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b541be6",
   "metadata": {},
   "source": [
    "Such that we we aim to reconstruct **WC** from **WF** and **FC** by multiplying them. For this, we typically initialize **WF** and **FC** with some random weights and attempt to multiply them to get **WC'** (an approximation of **WC**) and measure how close it is to **WC**. We do this multiple times using Stochastic Gradient Descent (SGD) to minimize the error. Finally, the **Word-Feature matrix (WF)** gives us the word embeddings for each word where **F** can be preset to a specific number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf9be1",
   "metadata": {},
   "source": [
    "# Robust Glove Model with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6443e",
   "metadata": {},
   "source": [
    "Let’s try and leverage GloVe based embeddings for our document clustering task. The very popular spacy framework comes with capabilities to leverage GloVe embeddings based on different language models. You can also get pre-trained word vectors and load them up as needed using gensim or spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a027d",
   "metadata": {},
   "source": [
    "If you have spacy installed, we will be using the **en_vectors_web_lg** model which consists of 300-dimensional word vectors trained on Common Crawl with GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f61251ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.4.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: setuptools in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dab9efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.4.0/en_core_web_trf-3.4.0-py3-none-any.whl (460.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 460.3 MB 62 kB/s  eta 0:00:01     |█████████████▊                  | 196.8 MB 3.3 MB/s eta 0:01:20     |███████████████████████████     | 386.9 MB 2.6 MB/s eta 0:00:29\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-trf==3.4.0) (3.4.0)\n",
      "Collecting spacy-transformers<1.2.0,>=1.1.2\n",
      "  Downloading spacy_transformers-1.1.7-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.21.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.0.4)\n",
      "Collecting transformers<4.21.0,>=3.4.0\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Downloading torch-1.12.0-cp39-none-macosx_10_9_x86_64.whl (133.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 133.6 MB 2.8 MB/s eta 0:00:01    |████████████████████▌           | 85.4 MB 2.9 MB/s eta 0:00:17\n",
      "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
      "  Downloading spacy_alignments-0.8.5-cp39-cp39-macosx_10_9_x86_64.whl (307 kB)\n",
      "\u001b[K     |████████████████████████████████| 307 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: filelock in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 3.3 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (2022.3.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.0) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-trf==3.4.0) (2.0.1)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers, torch, spacy-alignments, spacy-transformers, en-core-web-trf\n",
      "Successfully installed en-core-web-trf-3.4.0 huggingface-hub-0.8.1 spacy-alignments-0.8.5 spacy-transformers-1.1.7 tokenizers-0.12.1 torch-1.12.0 transformers-4.20.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fca8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting en-core-web-lg==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.0/en_core_web_lg-3.4.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 587.7 MB 14 kB/s  eta 0:00:01     |█████████▎                      | 169.7 MB 2.8 MB/s eta 0:02:28     |███████████████                 | 277.4 MB 2.8 MB/s eta 0:01:51     |████████████████                | 295.1 MB 3.4 MB/s eta 0:01:27     |█████████████████████████▊      | 472.5 MB 1.2 MB/s eta 0:01:34     |████████████████████████████████| 586.9 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-lg==3.4.0) (3.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: setuptools in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.21.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4747af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.4.0) (3.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: jinja2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b3046d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word vectors: 514157\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') # i can't use 'en_vectors_web_lg' (still don't know why)\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "print('Total word vectors:', total_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae42bc",
   "metadata": {},
   "source": [
    "This validates that everything is working and in order. Let’s get the GloVe embeddings for each of our words now in our toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90968146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>2.05650</td>\n",
       "      <td>-3.225900</td>\n",
       "      <td>-5.73640</td>\n",
       "      <td>-6.146000</td>\n",
       "      <td>0.15748</td>\n",
       "      <td>-2.428400</td>\n",
       "      <td>7.658000</td>\n",
       "      <td>2.706400</td>\n",
       "      <td>-2.211000</td>\n",
       "      <td>-0.899900</td>\n",
       "      <td>...</td>\n",
       "      <td>1.580200</td>\n",
       "      <td>1.75970</td>\n",
       "      <td>-0.60806</td>\n",
       "      <td>-6.61070</td>\n",
       "      <td>0.009383</td>\n",
       "      <td>-4.27630</td>\n",
       "      <td>-0.505070</td>\n",
       "      <td>5.00490</td>\n",
       "      <td>-8.53120</td>\n",
       "      <td>-1.49670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>2.63220</td>\n",
       "      <td>2.707800</td>\n",
       "      <td>-1.19830</td>\n",
       "      <td>2.149000</td>\n",
       "      <td>3.34010</td>\n",
       "      <td>-2.768000</td>\n",
       "      <td>0.285130</td>\n",
       "      <td>0.959040</td>\n",
       "      <td>-3.211000</td>\n",
       "      <td>0.978550</td>\n",
       "      <td>...</td>\n",
       "      <td>1.742100</td>\n",
       "      <td>-0.82167</td>\n",
       "      <td>2.79760</td>\n",
       "      <td>0.21357</td>\n",
       "      <td>1.293800</td>\n",
       "      <td>0.10757</td>\n",
       "      <td>0.832680</td>\n",
       "      <td>-0.80778</td>\n",
       "      <td>-1.53100</td>\n",
       "      <td>-0.28953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>-0.68660</td>\n",
       "      <td>-1.726700</td>\n",
       "      <td>-3.00130</td>\n",
       "      <td>-1.110100</td>\n",
       "      <td>1.83890</td>\n",
       "      <td>-3.076600</td>\n",
       "      <td>-0.233690</td>\n",
       "      <td>1.013300</td>\n",
       "      <td>-2.023400</td>\n",
       "      <td>3.033700</td>\n",
       "      <td>...</td>\n",
       "      <td>1.173600</td>\n",
       "      <td>-2.72720</td>\n",
       "      <td>1.57230</td>\n",
       "      <td>-2.42900</td>\n",
       "      <td>-1.527600</td>\n",
       "      <td>0.98387</td>\n",
       "      <td>-0.098775</td>\n",
       "      <td>2.95160</td>\n",
       "      <td>-1.60790</td>\n",
       "      <td>2.54120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-4.31020</td>\n",
       "      <td>2.570600</td>\n",
       "      <td>-3.47220</td>\n",
       "      <td>2.520500</td>\n",
       "      <td>-1.11040</td>\n",
       "      <td>-5.101400</td>\n",
       "      <td>-1.481200</td>\n",
       "      <td>4.655900</td>\n",
       "      <td>0.383650</td>\n",
       "      <td>2.477200</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.250500</td>\n",
       "      <td>4.20530</td>\n",
       "      <td>2.92490</td>\n",
       "      <td>-0.93310</td>\n",
       "      <td>0.890710</td>\n",
       "      <td>6.85240</td>\n",
       "      <td>0.075678</td>\n",
       "      <td>1.07370</td>\n",
       "      <td>-2.96000</td>\n",
       "      <td>-0.42209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1.23300</td>\n",
       "      <td>4.296300</td>\n",
       "      <td>-7.97380</td>\n",
       "      <td>-10.121000</td>\n",
       "      <td>1.82070</td>\n",
       "      <td>1.409800</td>\n",
       "      <td>-4.518000</td>\n",
       "      <td>-5.226100</td>\n",
       "      <td>-0.291570</td>\n",
       "      <td>0.952340</td>\n",
       "      <td>...</td>\n",
       "      <td>2.568800</td>\n",
       "      <td>-5.25470</td>\n",
       "      <td>6.98450</td>\n",
       "      <td>0.27835</td>\n",
       "      <td>-6.455400</td>\n",
       "      <td>-2.13270</td>\n",
       "      <td>-5.651500</td>\n",
       "      <td>11.17400</td>\n",
       "      <td>-8.05680</td>\n",
       "      <td>5.79850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>2.04220</td>\n",
       "      <td>3.634600</td>\n",
       "      <td>0.49222</td>\n",
       "      <td>0.074497</td>\n",
       "      <td>1.18540</td>\n",
       "      <td>2.842600</td>\n",
       "      <td>-0.042565</td>\n",
       "      <td>5.634800</td>\n",
       "      <td>0.948340</td>\n",
       "      <td>0.402610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538160</td>\n",
       "      <td>3.18240</td>\n",
       "      <td>1.99210</td>\n",
       "      <td>-1.43010</td>\n",
       "      <td>-0.164580</td>\n",
       "      <td>2.53300</td>\n",
       "      <td>-0.051668</td>\n",
       "      <td>1.00780</td>\n",
       "      <td>-0.29046</td>\n",
       "      <td>-1.68560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>-1.87590</td>\n",
       "      <td>-3.145100</td>\n",
       "      <td>0.26432</td>\n",
       "      <td>2.512800</td>\n",
       "      <td>5.12880</td>\n",
       "      <td>1.515500</td>\n",
       "      <td>-3.733400</td>\n",
       "      <td>3.628500</td>\n",
       "      <td>-0.975220</td>\n",
       "      <td>-1.097900</td>\n",
       "      <td>...</td>\n",
       "      <td>5.413500</td>\n",
       "      <td>0.45700</td>\n",
       "      <td>-3.24810</td>\n",
       "      <td>-1.46680</td>\n",
       "      <td>0.405470</td>\n",
       "      <td>3.04020</td>\n",
       "      <td>-1.674500</td>\n",
       "      <td>-3.53990</td>\n",
       "      <td>-4.88900</td>\n",
       "      <td>-2.04060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.18931</td>\n",
       "      <td>-0.706820</td>\n",
       "      <td>-3.17310</td>\n",
       "      <td>-4.140700</td>\n",
       "      <td>0.70243</td>\n",
       "      <td>-0.692550</td>\n",
       "      <td>2.348700</td>\n",
       "      <td>1.708300</td>\n",
       "      <td>-0.927380</td>\n",
       "      <td>3.484300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594240</td>\n",
       "      <td>2.14920</td>\n",
       "      <td>3.27380</td>\n",
       "      <td>-1.28600</td>\n",
       "      <td>1.950500</td>\n",
       "      <td>-0.67481</td>\n",
       "      <td>-0.930960</td>\n",
       "      <td>3.28140</td>\n",
       "      <td>-5.52320</td>\n",
       "      <td>-0.31346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.93585</td>\n",
       "      <td>-0.450810</td>\n",
       "      <td>-4.95600</td>\n",
       "      <td>2.254800</td>\n",
       "      <td>1.21050</td>\n",
       "      <td>-1.667600</td>\n",
       "      <td>0.879590</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>-4.167800</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.116600</td>\n",
       "      <td>1.10070</td>\n",
       "      <td>4.04960</td>\n",
       "      <td>-0.97496</td>\n",
       "      <td>-5.127300</td>\n",
       "      <td>-0.63158</td>\n",
       "      <td>6.890100</td>\n",
       "      <td>-0.34744</td>\n",
       "      <td>2.44190</td>\n",
       "      <td>0.90073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>-1.93390</td>\n",
       "      <td>0.314870</td>\n",
       "      <td>-4.39510</td>\n",
       "      <td>1.924700</td>\n",
       "      <td>-2.04800</td>\n",
       "      <td>-5.712100</td>\n",
       "      <td>-0.956200</td>\n",
       "      <td>2.792700</td>\n",
       "      <td>-1.307100</td>\n",
       "      <td>3.660400</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.795100</td>\n",
       "      <td>0.43909</td>\n",
       "      <td>2.74080</td>\n",
       "      <td>-2.18080</td>\n",
       "      <td>-0.936220</td>\n",
       "      <td>1.71610</td>\n",
       "      <td>4.210300</td>\n",
       "      <td>-0.44140</td>\n",
       "      <td>-3.99350</td>\n",
       "      <td>-0.57955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>1.53330</td>\n",
       "      <td>1.622600</td>\n",
       "      <td>1.05520</td>\n",
       "      <td>-1.361500</td>\n",
       "      <td>0.97952</td>\n",
       "      <td>-0.828060</td>\n",
       "      <td>1.485400</td>\n",
       "      <td>2.881700</td>\n",
       "      <td>-0.354000</td>\n",
       "      <td>0.344570</td>\n",
       "      <td>...</td>\n",
       "      <td>3.243400</td>\n",
       "      <td>-1.53780</td>\n",
       "      <td>2.36610</td>\n",
       "      <td>-2.51120</td>\n",
       "      <td>-1.267200</td>\n",
       "      <td>-1.48080</td>\n",
       "      <td>-1.282500</td>\n",
       "      <td>1.41430</td>\n",
       "      <td>-2.50450</td>\n",
       "      <td>2.77440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.56940</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>-2.79580</td>\n",
       "      <td>-6.990900</td>\n",
       "      <td>-1.94450</td>\n",
       "      <td>2.147300</td>\n",
       "      <td>-4.308900</td>\n",
       "      <td>-2.577500</td>\n",
       "      <td>3.211000</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050673</td>\n",
       "      <td>1.79860</td>\n",
       "      <td>6.77940</td>\n",
       "      <td>-2.54120</td>\n",
       "      <td>-0.408370</td>\n",
       "      <td>3.17150</td>\n",
       "      <td>0.974170</td>\n",
       "      <td>5.32660</td>\n",
       "      <td>-10.14900</td>\n",
       "      <td>-0.31148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.94699</td>\n",
       "      <td>-3.802500</td>\n",
       "      <td>-5.22820</td>\n",
       "      <td>1.638600</td>\n",
       "      <td>4.74710</td>\n",
       "      <td>-3.459600</td>\n",
       "      <td>-2.239200</td>\n",
       "      <td>0.847490</td>\n",
       "      <td>-3.645700</td>\n",
       "      <td>3.273800</td>\n",
       "      <td>...</td>\n",
       "      <td>6.575500</td>\n",
       "      <td>1.13930</td>\n",
       "      <td>-2.46300</td>\n",
       "      <td>-7.98480</td>\n",
       "      <td>-7.093400</td>\n",
       "      <td>-2.87130</td>\n",
       "      <td>3.126500</td>\n",
       "      <td>3.40870</td>\n",
       "      <td>-4.73420</td>\n",
       "      <td>-0.91956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>7.15240</td>\n",
       "      <td>3.035600</td>\n",
       "      <td>-8.70400</td>\n",
       "      <td>0.937880</td>\n",
       "      <td>-3.67810</td>\n",
       "      <td>-0.035128</td>\n",
       "      <td>1.256800</td>\n",
       "      <td>0.024432</td>\n",
       "      <td>0.289240</td>\n",
       "      <td>4.505400</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.771200</td>\n",
       "      <td>4.76440</td>\n",
       "      <td>4.23500</td>\n",
       "      <td>-1.84270</td>\n",
       "      <td>0.369470</td>\n",
       "      <td>1.31650</td>\n",
       "      <td>1.165000</td>\n",
       "      <td>-1.39280</td>\n",
       "      <td>-3.91250</td>\n",
       "      <td>0.48484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.22776</td>\n",
       "      <td>-3.051600</td>\n",
       "      <td>0.26025</td>\n",
       "      <td>0.641570</td>\n",
       "      <td>2.04230</td>\n",
       "      <td>-3.136500</td>\n",
       "      <td>0.128790</td>\n",
       "      <td>0.181220</td>\n",
       "      <td>-1.535200</td>\n",
       "      <td>0.290250</td>\n",
       "      <td>...</td>\n",
       "      <td>3.504000</td>\n",
       "      <td>-1.27140</td>\n",
       "      <td>4.07630</td>\n",
       "      <td>-5.66470</td>\n",
       "      <td>0.247550</td>\n",
       "      <td>1.58200</td>\n",
       "      <td>3.224800</td>\n",
       "      <td>3.00600</td>\n",
       "      <td>2.78730</td>\n",
       "      <td>2.65260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-1.58880</td>\n",
       "      <td>0.733160</td>\n",
       "      <td>1.07460</td>\n",
       "      <td>-2.452100</td>\n",
       "      <td>-0.42517</td>\n",
       "      <td>3.334400</td>\n",
       "      <td>-0.179420</td>\n",
       "      <td>-1.092400</td>\n",
       "      <td>-0.093904</td>\n",
       "      <td>0.058663</td>\n",
       "      <td>...</td>\n",
       "      <td>1.125300</td>\n",
       "      <td>-2.35530</td>\n",
       "      <td>3.93600</td>\n",
       "      <td>-3.37510</td>\n",
       "      <td>-0.999340</td>\n",
       "      <td>1.26390</td>\n",
       "      <td>-2.106800</td>\n",
       "      <td>3.07430</td>\n",
       "      <td>-4.42900</td>\n",
       "      <td>3.89380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>-0.95324</td>\n",
       "      <td>-3.505800</td>\n",
       "      <td>-1.52990</td>\n",
       "      <td>2.149900</td>\n",
       "      <td>3.81270</td>\n",
       "      <td>-1.850400</td>\n",
       "      <td>-0.960970</td>\n",
       "      <td>1.255800</td>\n",
       "      <td>-2.262700</td>\n",
       "      <td>1.484200</td>\n",
       "      <td>...</td>\n",
       "      <td>3.824700</td>\n",
       "      <td>-0.62691</td>\n",
       "      <td>2.76300</td>\n",
       "      <td>-5.66990</td>\n",
       "      <td>-0.950370</td>\n",
       "      <td>-0.58964</td>\n",
       "      <td>2.101200</td>\n",
       "      <td>1.12920</td>\n",
       "      <td>0.98625</td>\n",
       "      <td>0.59990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>-1.93140</td>\n",
       "      <td>0.078516</td>\n",
       "      <td>1.47560</td>\n",
       "      <td>1.597600</td>\n",
       "      <td>1.57150</td>\n",
       "      <td>-2.340500</td>\n",
       "      <td>0.164920</td>\n",
       "      <td>0.763640</td>\n",
       "      <td>-1.061700</td>\n",
       "      <td>3.975100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.873200</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>-1.02720</td>\n",
       "      <td>-0.16442</td>\n",
       "      <td>-1.481600</td>\n",
       "      <td>-1.98390</td>\n",
       "      <td>1.898100</td>\n",
       "      <td>2.54110</td>\n",
       "      <td>2.28320</td>\n",
       "      <td>1.28470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.44278</td>\n",
       "      <td>-4.114100</td>\n",
       "      <td>1.23560</td>\n",
       "      <td>4.187300</td>\n",
       "      <td>1.62440</td>\n",
       "      <td>-1.260100</td>\n",
       "      <td>-1.643500</td>\n",
       "      <td>-0.900330</td>\n",
       "      <td>-1.241500</td>\n",
       "      <td>0.867240</td>\n",
       "      <td>...</td>\n",
       "      <td>3.657400</td>\n",
       "      <td>-1.63910</td>\n",
       "      <td>3.90300</td>\n",
       "      <td>-6.56120</td>\n",
       "      <td>3.673700</td>\n",
       "      <td>-1.20580</td>\n",
       "      <td>3.819300</td>\n",
       "      <td>3.60500</td>\n",
       "      <td>3.63860</td>\n",
       "      <td>4.03620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>-3.84290</td>\n",
       "      <td>0.140680</td>\n",
       "      <td>-3.35840</td>\n",
       "      <td>3.267900</td>\n",
       "      <td>-2.15360</td>\n",
       "      <td>-6.508500</td>\n",
       "      <td>0.235120</td>\n",
       "      <td>6.884500</td>\n",
       "      <td>-0.543490</td>\n",
       "      <td>3.596900</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.592000</td>\n",
       "      <td>1.02690</td>\n",
       "      <td>1.60640</td>\n",
       "      <td>-2.84040</td>\n",
       "      <td>-2.274700</td>\n",
       "      <td>2.10020</td>\n",
       "      <td>5.480200</td>\n",
       "      <td>0.83172</td>\n",
       "      <td>-4.30830</td>\n",
       "      <td>-1.04370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1        2          3        4         5         6    \\\n",
       "love       2.05650 -3.225900 -5.73640  -6.146000  0.15748 -2.428400  7.658000   \n",
       "quick      2.63220  2.707800 -1.19830   2.149000  3.34010 -2.768000  0.285130   \n",
       "breakfast -0.68660 -1.726700 -3.00130  -1.110100  1.83890 -3.076600 -0.233690   \n",
       "blue      -4.31020  2.570600 -3.47220   2.520500 -1.11040 -5.101400 -1.481200   \n",
       "dog        1.23300  4.296300 -7.97380 -10.121000  1.82070  1.409800 -4.518000   \n",
       "jumps      2.04220  3.634600  0.49222   0.074497  1.18540  2.842600 -0.042565   \n",
       "kings     -1.87590 -3.145100  0.26432   2.512800  5.12880  1.515500 -3.733400   \n",
       "beautiful -0.18931 -0.706820 -3.17310  -4.140700  0.70243 -0.692550  2.348700   \n",
       "beans      0.93585 -0.450810 -4.95600   2.254800  1.21050 -1.667600  0.879590   \n",
       "green     -1.93390  0.314870 -4.39510   1.924700 -2.04800 -5.712100 -0.956200   \n",
       "today      1.53330  1.622600  1.05520  -1.361500  0.97952 -0.828060  1.485400   \n",
       "fox        0.56940  1.003500 -2.79580  -6.990900 -1.94450  2.147300 -4.308900   \n",
       "eggs       0.94699 -3.802500 -5.22820   1.638600  4.74710 -3.459600 -2.239200   \n",
       "sky        7.15240  3.035600 -8.70400   0.937880 -3.67810 -0.035128  1.256800   \n",
       "bacon      0.22776 -3.051600  0.26025   0.641570  2.04230 -3.136500  0.128790   \n",
       "lazy      -1.58880  0.733160  1.07460  -2.452100 -0.42517  3.334400 -0.179420   \n",
       "sausages  -0.95324 -3.505800 -1.52990   2.149900  3.81270 -1.850400 -0.960970   \n",
       "toast     -1.93140  0.078516  1.47560   1.597600  1.57150 -2.340500  0.164920   \n",
       "ham        0.44278 -4.114100  1.23560   4.187300  1.62440 -1.260100 -1.643500   \n",
       "brown     -3.84290  0.140680 -3.35840   3.267900 -2.15360 -6.508500  0.235120   \n",
       "\n",
       "                7         8         9    ...       290      291      292  \\\n",
       "love       2.706400 -2.211000 -0.899900  ...  1.580200  1.75970 -0.60806   \n",
       "quick      0.959040 -3.211000  0.978550  ...  1.742100 -0.82167  2.79760   \n",
       "breakfast  1.013300 -2.023400  3.033700  ...  1.173600 -2.72720  1.57230   \n",
       "blue       4.655900  0.383650  2.477200  ... -5.250500  4.20530  2.92490   \n",
       "dog       -5.226100 -0.291570  0.952340  ...  2.568800 -5.25470  6.98450   \n",
       "jumps      5.634800  0.948340  0.402610  ... -0.538160  3.18240  1.99210   \n",
       "kings      3.628500 -0.975220 -1.097900  ...  5.413500  0.45700 -3.24810   \n",
       "beautiful  1.708300 -0.927380  3.484300  ...  0.594240  2.14920  3.27380   \n",
       "beans      0.004286 -4.167800  3.960000  ...  3.116600  1.10070  4.04960   \n",
       "green      2.792700 -1.307100  3.660400  ... -2.795100  0.43909  2.74080   \n",
       "today      2.881700 -0.354000  0.344570  ...  3.243400 -1.53780  2.36610   \n",
       "fox       -2.577500  3.211000  0.202600  ... -0.050673  1.79860  6.77940   \n",
       "eggs       0.847490 -3.645700  3.273800  ...  6.575500  1.13930 -2.46300   \n",
       "sky        0.024432  0.289240  4.505400  ... -5.771200  4.76440  4.23500   \n",
       "bacon      0.181220 -1.535200  0.290250  ...  3.504000 -1.27140  4.07630   \n",
       "lazy      -1.092400 -0.093904  0.058663  ...  1.125300 -2.35530  3.93600   \n",
       "sausages   1.255800 -2.262700  1.484200  ...  3.824700 -0.62691  2.76300   \n",
       "toast      0.763640 -1.061700  3.975100  ...  1.873200  0.96658 -1.02720   \n",
       "ham       -0.900330 -1.241500  0.867240  ...  3.657400 -1.63910  3.90300   \n",
       "brown      6.884500 -0.543490  3.596900  ... -1.592000  1.02690  1.60640   \n",
       "\n",
       "               293       294      295       296       297       298      299  \n",
       "love      -6.61070  0.009383 -4.27630 -0.505070   5.00490  -8.53120 -1.49670  \n",
       "quick      0.21357  1.293800  0.10757  0.832680  -0.80778  -1.53100 -0.28953  \n",
       "breakfast -2.42900 -1.527600  0.98387 -0.098775   2.95160  -1.60790  2.54120  \n",
       "blue      -0.93310  0.890710  6.85240  0.075678   1.07370  -2.96000 -0.42209  \n",
       "dog        0.27835 -6.455400 -2.13270 -5.651500  11.17400  -8.05680  5.79850  \n",
       "jumps     -1.43010 -0.164580  2.53300 -0.051668   1.00780  -0.29046 -1.68560  \n",
       "kings     -1.46680  0.405470  3.04020 -1.674500  -3.53990  -4.88900 -2.04060  \n",
       "beautiful -1.28600  1.950500 -0.67481 -0.930960   3.28140  -5.52320 -0.31346  \n",
       "beans     -0.97496 -5.127300 -0.63158  6.890100  -0.34744   2.44190  0.90073  \n",
       "green     -2.18080 -0.936220  1.71610  4.210300  -0.44140  -3.99350 -0.57955  \n",
       "today     -2.51120 -1.267200 -1.48080 -1.282500   1.41430  -2.50450  2.77440  \n",
       "fox       -2.54120 -0.408370  3.17150  0.974170   5.32660 -10.14900 -0.31148  \n",
       "eggs      -7.98480 -7.093400 -2.87130  3.126500   3.40870  -4.73420 -0.91956  \n",
       "sky       -1.84270  0.369470  1.31650  1.165000  -1.39280  -3.91250  0.48484  \n",
       "bacon     -5.66470  0.247550  1.58200  3.224800   3.00600   2.78730  2.65260  \n",
       "lazy      -3.37510 -0.999340  1.26390 -2.106800   3.07430  -4.42900  3.89380  \n",
       "sausages  -5.66990 -0.950370 -0.58964  2.101200   1.12920   0.98625  0.59990  \n",
       "toast     -0.16442 -1.481600 -1.98390  1.898100   2.54110   2.28320  1.28470  \n",
       "ham       -6.56120  3.673700 -1.20580  3.819300   3.60500   3.63860  4.03620  \n",
       "brown     -2.84040 -2.274700  2.10020  5.480200   0.83172  -4.30830 -1.04370  \n",
       "\n",
       "[20 rows x 300 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set([word for sublist in tokenized_corpus for word in sublist]))\n",
    "\n",
    "word_glove_vectors = np.array([nlp(word).vector for word in unique_words])\n",
    "vec_df = pd.DataFrame(word_glove_vectors, index=unique_words)\n",
    "vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fac5d",
   "metadata": {},
   "source": [
    "We can now use t-SNE to visualize these embeddings similar to what we did using our Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22aec6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_glove_vectors)\n",
    "labels = unique_words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='red', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7500566",
   "metadata": {},
   "source": [
    "## Looking at term semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "076feff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>blue</th>\n",
       "      <th>dog</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>beans</th>\n",
       "      <th>green</th>\n",
       "      <th>today</th>\n",
       "      <th>fox</th>\n",
       "      <th>eggs</th>\n",
       "      <th>sky</th>\n",
       "      <th>bacon</th>\n",
       "      <th>lazy</th>\n",
       "      <th>sausages</th>\n",
       "      <th>toast</th>\n",
       "      <th>ham</th>\n",
       "      <th>brown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.179513</td>\n",
       "      <td>0.248741</td>\n",
       "      <td>0.174761</td>\n",
       "      <td>0.233190</td>\n",
       "      <td>0.096790</td>\n",
       "      <td>0.174145</td>\n",
       "      <td>0.583773</td>\n",
       "      <td>0.186205</td>\n",
       "      <td>0.188962</td>\n",
       "      <td>0.284772</td>\n",
       "      <td>0.079268</td>\n",
       "      <td>0.143901</td>\n",
       "      <td>0.199313</td>\n",
       "      <td>0.115246</td>\n",
       "      <td>0.165050</td>\n",
       "      <td>0.188998</td>\n",
       "      <td>0.194567</td>\n",
       "      <td>0.057496</td>\n",
       "      <td>0.202454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>0.179513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.329268</td>\n",
       "      <td>0.082821</td>\n",
       "      <td>0.212741</td>\n",
       "      <td>0.261009</td>\n",
       "      <td>0.055522</td>\n",
       "      <td>0.257739</td>\n",
       "      <td>0.202361</td>\n",
       "      <td>0.212049</td>\n",
       "      <td>0.283282</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.102922</td>\n",
       "      <td>0.028834</td>\n",
       "      <td>0.114120</td>\n",
       "      <td>0.232939</td>\n",
       "      <td>0.237747</td>\n",
       "      <td>0.142354</td>\n",
       "      <td>0.066001</td>\n",
       "      <td>0.144577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>0.248741</td>\n",
       "      <td>0.329268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.126405</td>\n",
       "      <td>0.269188</td>\n",
       "      <td>0.098489</td>\n",
       "      <td>0.161264</td>\n",
       "      <td>0.376880</td>\n",
       "      <td>0.400320</td>\n",
       "      <td>0.264153</td>\n",
       "      <td>0.383621</td>\n",
       "      <td>-0.013405</td>\n",
       "      <td>0.379243</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.466563</td>\n",
       "      <td>0.185971</td>\n",
       "      <td>0.518220</td>\n",
       "      <td>0.558907</td>\n",
       "      <td>0.386419</td>\n",
       "      <td>0.270948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.174761</td>\n",
       "      <td>0.082821</td>\n",
       "      <td>0.126405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135320</td>\n",
       "      <td>0.157356</td>\n",
       "      <td>0.157707</td>\n",
       "      <td>0.405862</td>\n",
       "      <td>0.214204</td>\n",
       "      <td>0.744065</td>\n",
       "      <td>0.097223</td>\n",
       "      <td>0.308713</td>\n",
       "      <td>0.129324</td>\n",
       "      <td>0.469081</td>\n",
       "      <td>0.149656</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.156515</td>\n",
       "      <td>0.129439</td>\n",
       "      <td>0.079979</td>\n",
       "      <td>0.671114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.233190</td>\n",
       "      <td>0.212741</td>\n",
       "      <td>0.269188</td>\n",
       "      <td>0.135320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.218082</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.200093</td>\n",
       "      <td>0.193062</td>\n",
       "      <td>0.095792</td>\n",
       "      <td>0.083778</td>\n",
       "      <td>0.505732</td>\n",
       "      <td>0.231863</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.185360</td>\n",
       "      <td>0.197494</td>\n",
       "      <td>0.237239</td>\n",
       "      <td>0.097297</td>\n",
       "      <td>0.162691</td>\n",
       "      <td>0.209964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>0.096790</td>\n",
       "      <td>0.261009</td>\n",
       "      <td>0.098489</td>\n",
       "      <td>0.157356</td>\n",
       "      <td>0.218082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.101878</td>\n",
       "      <td>0.078380</td>\n",
       "      <td>0.118674</td>\n",
       "      <td>0.024164</td>\n",
       "      <td>0.215905</td>\n",
       "      <td>0.137903</td>\n",
       "      <td>0.274471</td>\n",
       "      <td>-0.012451</td>\n",
       "      <td>0.097227</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>0.029780</td>\n",
       "      <td>-0.037652</td>\n",
       "      <td>0.102126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.174145</td>\n",
       "      <td>0.055522</td>\n",
       "      <td>0.161264</td>\n",
       "      <td>0.157707</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.244985</td>\n",
       "      <td>0.125885</td>\n",
       "      <td>0.156652</td>\n",
       "      <td>0.219406</td>\n",
       "      <td>0.094101</td>\n",
       "      <td>0.149547</td>\n",
       "      <td>0.152930</td>\n",
       "      <td>0.069831</td>\n",
       "      <td>-0.022814</td>\n",
       "      <td>0.311361</td>\n",
       "      <td>0.108454</td>\n",
       "      <td>-0.018844</td>\n",
       "      <td>0.177645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>0.583773</td>\n",
       "      <td>0.257739</td>\n",
       "      <td>0.376880</td>\n",
       "      <td>0.405862</td>\n",
       "      <td>0.200093</td>\n",
       "      <td>0.101878</td>\n",
       "      <td>0.244985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.219528</td>\n",
       "      <td>0.474428</td>\n",
       "      <td>0.379662</td>\n",
       "      <td>0.155214</td>\n",
       "      <td>0.167463</td>\n",
       "      <td>0.317680</td>\n",
       "      <td>0.119129</td>\n",
       "      <td>0.160029</td>\n",
       "      <td>0.254617</td>\n",
       "      <td>0.238517</td>\n",
       "      <td>0.065874</td>\n",
       "      <td>0.375549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.186205</td>\n",
       "      <td>0.202361</td>\n",
       "      <td>0.400320</td>\n",
       "      <td>0.214204</td>\n",
       "      <td>0.193062</td>\n",
       "      <td>0.078380</td>\n",
       "      <td>0.125885</td>\n",
       "      <td>0.219528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392007</td>\n",
       "      <td>0.108890</td>\n",
       "      <td>0.096549</td>\n",
       "      <td>0.460948</td>\n",
       "      <td>0.103234</td>\n",
       "      <td>0.511582</td>\n",
       "      <td>0.102601</td>\n",
       "      <td>0.604881</td>\n",
       "      <td>0.435954</td>\n",
       "      <td>0.443734</td>\n",
       "      <td>0.459392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.188962</td>\n",
       "      <td>0.212049</td>\n",
       "      <td>0.264153</td>\n",
       "      <td>0.744065</td>\n",
       "      <td>0.095792</td>\n",
       "      <td>0.118674</td>\n",
       "      <td>0.156652</td>\n",
       "      <td>0.474428</td>\n",
       "      <td>0.392007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176654</td>\n",
       "      <td>0.171780</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>0.355054</td>\n",
       "      <td>0.204442</td>\n",
       "      <td>0.043046</td>\n",
       "      <td>0.270824</td>\n",
       "      <td>0.183584</td>\n",
       "      <td>0.115664</td>\n",
       "      <td>0.677107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.284772</td>\n",
       "      <td>0.283282</td>\n",
       "      <td>0.383621</td>\n",
       "      <td>0.097223</td>\n",
       "      <td>0.083778</td>\n",
       "      <td>0.024164</td>\n",
       "      <td>0.219406</td>\n",
       "      <td>0.379662</td>\n",
       "      <td>0.108890</td>\n",
       "      <td>0.176654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010242</td>\n",
       "      <td>0.064808</td>\n",
       "      <td>0.109907</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.032521</td>\n",
       "      <td>0.195929</td>\n",
       "      <td>0.127586</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.079016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.079268</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>-0.013405</td>\n",
       "      <td>0.308713</td>\n",
       "      <td>0.505732</td>\n",
       "      <td>0.215905</td>\n",
       "      <td>0.094101</td>\n",
       "      <td>0.155214</td>\n",
       "      <td>0.096549</td>\n",
       "      <td>0.171780</td>\n",
       "      <td>-0.010242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216031</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.117890</td>\n",
       "      <td>0.199506</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.034016</td>\n",
       "      <td>0.153249</td>\n",
       "      <td>0.283868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.143901</td>\n",
       "      <td>0.102922</td>\n",
       "      <td>0.379243</td>\n",
       "      <td>0.129324</td>\n",
       "      <td>0.231863</td>\n",
       "      <td>0.137903</td>\n",
       "      <td>0.149547</td>\n",
       "      <td>0.167463</td>\n",
       "      <td>0.460948</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>0.064808</td>\n",
       "      <td>0.216031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109845</td>\n",
       "      <td>0.484215</td>\n",
       "      <td>0.090397</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>0.387336</td>\n",
       "      <td>0.412742</td>\n",
       "      <td>0.399797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>0.199313</td>\n",
       "      <td>0.028834</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.469081</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.274471</td>\n",
       "      <td>0.152930</td>\n",
       "      <td>0.317680</td>\n",
       "      <td>0.103234</td>\n",
       "      <td>0.355054</td>\n",
       "      <td>0.109907</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.109845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045584</td>\n",
       "      <td>0.071306</td>\n",
       "      <td>0.035162</td>\n",
       "      <td>0.128492</td>\n",
       "      <td>0.009258</td>\n",
       "      <td>0.246960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.115246</td>\n",
       "      <td>0.114120</td>\n",
       "      <td>0.466563</td>\n",
       "      <td>0.149656</td>\n",
       "      <td>0.185360</td>\n",
       "      <td>-0.012451</td>\n",
       "      <td>0.069831</td>\n",
       "      <td>0.119129</td>\n",
       "      <td>0.511582</td>\n",
       "      <td>0.204442</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.117890</td>\n",
       "      <td>0.484215</td>\n",
       "      <td>0.045584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.158482</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.619755</td>\n",
       "      <td>0.787793</td>\n",
       "      <td>0.374713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>0.165050</td>\n",
       "      <td>0.232939</td>\n",
       "      <td>0.185971</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.197494</td>\n",
       "      <td>0.097227</td>\n",
       "      <td>-0.022814</td>\n",
       "      <td>0.160029</td>\n",
       "      <td>0.102601</td>\n",
       "      <td>0.043046</td>\n",
       "      <td>0.032521</td>\n",
       "      <td>0.199506</td>\n",
       "      <td>0.090397</td>\n",
       "      <td>0.071306</td>\n",
       "      <td>0.158482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099845</td>\n",
       "      <td>0.176360</td>\n",
       "      <td>0.184291</td>\n",
       "      <td>0.121795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>0.188998</td>\n",
       "      <td>0.237747</td>\n",
       "      <td>0.518220</td>\n",
       "      <td>0.156515</td>\n",
       "      <td>0.237239</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>0.311361</td>\n",
       "      <td>0.254617</td>\n",
       "      <td>0.604881</td>\n",
       "      <td>0.270824</td>\n",
       "      <td>0.195929</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>0.035162</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.099845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465966</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>0.405929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>0.194567</td>\n",
       "      <td>0.142354</td>\n",
       "      <td>0.558907</td>\n",
       "      <td>0.129439</td>\n",
       "      <td>0.097297</td>\n",
       "      <td>0.029780</td>\n",
       "      <td>0.108454</td>\n",
       "      <td>0.238517</td>\n",
       "      <td>0.435954</td>\n",
       "      <td>0.183584</td>\n",
       "      <td>0.127586</td>\n",
       "      <td>0.034016</td>\n",
       "      <td>0.387336</td>\n",
       "      <td>0.128492</td>\n",
       "      <td>0.619755</td>\n",
       "      <td>0.176360</td>\n",
       "      <td>0.465966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.564325</td>\n",
       "      <td>0.282405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.057496</td>\n",
       "      <td>0.066001</td>\n",
       "      <td>0.386419</td>\n",
       "      <td>0.079979</td>\n",
       "      <td>0.162691</td>\n",
       "      <td>-0.037652</td>\n",
       "      <td>-0.018844</td>\n",
       "      <td>0.065874</td>\n",
       "      <td>0.443734</td>\n",
       "      <td>0.115664</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.153249</td>\n",
       "      <td>0.412742</td>\n",
       "      <td>0.009258</td>\n",
       "      <td>0.787793</td>\n",
       "      <td>0.184291</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>0.564325</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.264582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>0.202454</td>\n",
       "      <td>0.144577</td>\n",
       "      <td>0.270948</td>\n",
       "      <td>0.671114</td>\n",
       "      <td>0.209964</td>\n",
       "      <td>0.102126</td>\n",
       "      <td>0.177645</td>\n",
       "      <td>0.375549</td>\n",
       "      <td>0.459392</td>\n",
       "      <td>0.677107</td>\n",
       "      <td>0.079016</td>\n",
       "      <td>0.283868</td>\n",
       "      <td>0.399797</td>\n",
       "      <td>0.246960</td>\n",
       "      <td>0.374713</td>\n",
       "      <td>0.121795</td>\n",
       "      <td>0.405929</td>\n",
       "      <td>0.282405</td>\n",
       "      <td>0.264582</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               love     quick  breakfast      blue       dog     jumps  \\\n",
       "love       1.000000  0.179513   0.248741  0.174761  0.233190  0.096790   \n",
       "quick      0.179513  1.000000   0.329268  0.082821  0.212741  0.261009   \n",
       "breakfast  0.248741  0.329268   1.000000  0.126405  0.269188  0.098489   \n",
       "blue       0.174761  0.082821   0.126405  1.000000  0.135320  0.157356   \n",
       "dog        0.233190  0.212741   0.269188  0.135320  1.000000  0.218082   \n",
       "jumps      0.096790  0.261009   0.098489  0.157356  0.218082  1.000000   \n",
       "kings      0.174145  0.055522   0.161264  0.157707  0.012827  0.016656   \n",
       "beautiful  0.583773  0.257739   0.376880  0.405862  0.200093  0.101878   \n",
       "beans      0.186205  0.202361   0.400320  0.214204  0.193062  0.078380   \n",
       "green      0.188962  0.212049   0.264153  0.744065  0.095792  0.118674   \n",
       "today      0.284772  0.283282   0.383621  0.097223  0.083778  0.024164   \n",
       "fox        0.079268  0.003717  -0.013405  0.308713  0.505732  0.215905   \n",
       "eggs       0.143901  0.102922   0.379243  0.129324  0.231863  0.137903   \n",
       "sky        0.199313  0.028834   0.079245  0.469081  0.064700  0.274471   \n",
       "bacon      0.115246  0.114120   0.466563  0.149656  0.185360 -0.012451   \n",
       "lazy       0.165050  0.232939   0.185971  0.060606  0.197494  0.097227   \n",
       "sausages   0.188998  0.237747   0.518220  0.156515  0.237239  0.057436   \n",
       "toast      0.194567  0.142354   0.558907  0.129439  0.097297  0.029780   \n",
       "ham        0.057496  0.066001   0.386419  0.079979  0.162691 -0.037652   \n",
       "brown      0.202454  0.144577   0.270948  0.671114  0.209964  0.102126   \n",
       "\n",
       "              kings  beautiful     beans     green     today       fox  \\\n",
       "love       0.174145   0.583773  0.186205  0.188962  0.284772  0.079268   \n",
       "quick      0.055522   0.257739  0.202361  0.212049  0.283282  0.003717   \n",
       "breakfast  0.161264   0.376880  0.400320  0.264153  0.383621 -0.013405   \n",
       "blue       0.157707   0.405862  0.214204  0.744065  0.097223  0.308713   \n",
       "dog        0.012827   0.200093  0.193062  0.095792  0.083778  0.505732   \n",
       "jumps      0.016656   0.101878  0.078380  0.118674  0.024164  0.215905   \n",
       "kings      1.000000   0.244985  0.125885  0.156652  0.219406  0.094101   \n",
       "beautiful  0.244985   1.000000  0.219528  0.474428  0.379662  0.155214   \n",
       "beans      0.125885   0.219528  1.000000  0.392007  0.108890  0.096549   \n",
       "green      0.156652   0.474428  0.392007  1.000000  0.176654  0.171780   \n",
       "today      0.219406   0.379662  0.108890  0.176654  1.000000 -0.010242   \n",
       "fox        0.094101   0.155214  0.096549  0.171780 -0.010242  1.000000   \n",
       "eggs       0.149547   0.167463  0.460948  0.226500  0.064808  0.216031   \n",
       "sky        0.152930   0.317680  0.103234  0.355054  0.109907  0.267669   \n",
       "bacon      0.069831   0.119129  0.511582  0.204442  0.053974  0.117890   \n",
       "lazy      -0.022814   0.160029  0.102601  0.043046  0.032521  0.199506   \n",
       "sausages   0.311361   0.254617  0.604881  0.270824  0.195929  0.101075   \n",
       "toast      0.108454   0.238517  0.435954  0.183584  0.127586  0.034016   \n",
       "ham       -0.018844   0.065874  0.443734  0.115664  0.001170  0.153249   \n",
       "brown      0.177645   0.375549  0.459392  0.677107  0.079016  0.283868   \n",
       "\n",
       "               eggs       sky     bacon      lazy  sausages     toast  \\\n",
       "love       0.143901  0.199313  0.115246  0.165050  0.188998  0.194567   \n",
       "quick      0.102922  0.028834  0.114120  0.232939  0.237747  0.142354   \n",
       "breakfast  0.379243  0.079245  0.466563  0.185971  0.518220  0.558907   \n",
       "blue       0.129324  0.469081  0.149656  0.060606  0.156515  0.129439   \n",
       "dog        0.231863  0.064700  0.185360  0.197494  0.237239  0.097297   \n",
       "jumps      0.137903  0.274471 -0.012451  0.097227  0.057436  0.029780   \n",
       "kings      0.149547  0.152930  0.069831 -0.022814  0.311361  0.108454   \n",
       "beautiful  0.167463  0.317680  0.119129  0.160029  0.254617  0.238517   \n",
       "beans      0.460948  0.103234  0.511582  0.102601  0.604881  0.435954   \n",
       "green      0.226500  0.355054  0.204442  0.043046  0.270824  0.183584   \n",
       "today      0.064808  0.109907  0.053974  0.032521  0.195929  0.127586   \n",
       "fox        0.216031  0.267669  0.117890  0.199506  0.101075  0.034016   \n",
       "eggs       1.000000  0.109845  0.484215  0.090397  0.534306  0.387336   \n",
       "sky        0.109845  1.000000  0.045584  0.071306  0.035162  0.128492   \n",
       "bacon      0.484215  0.045584  1.000000  0.158482  0.693694  0.619755   \n",
       "lazy       0.090397  0.071306  0.158482  1.000000  0.099845  0.176360   \n",
       "sausages   0.534306  0.035162  0.693694  0.099845  1.000000  0.465966   \n",
       "toast      0.387336  0.128492  0.619755  0.176360  0.465966  1.000000   \n",
       "ham        0.412742  0.009258  0.787793  0.184291  0.647558  0.564325   \n",
       "brown      0.399797  0.246960  0.374713  0.121795  0.405929  0.282405   \n",
       "\n",
       "                ham     brown  \n",
       "love       0.057496  0.202454  \n",
       "quick      0.066001  0.144577  \n",
       "breakfast  0.386419  0.270948  \n",
       "blue       0.079979  0.671114  \n",
       "dog        0.162691  0.209964  \n",
       "jumps     -0.037652  0.102126  \n",
       "kings     -0.018844  0.177645  \n",
       "beautiful  0.065874  0.375549  \n",
       "beans      0.443734  0.459392  \n",
       "green      0.115664  0.677107  \n",
       "today      0.001170  0.079016  \n",
       "fox        0.153249  0.283868  \n",
       "eggs       0.412742  0.399797  \n",
       "sky        0.009258  0.246960  \n",
       "bacon      0.787793  0.374713  \n",
       "lazy       0.184291  0.121795  \n",
       "sausages   0.647558  0.405929  \n",
       "toast      0.564325  0.282405  \n",
       "ham        1.000000  0.264582  \n",
       "brown      0.264582  1.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(vec_df.values)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=unique_words, columns=unique_words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7087516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "love         [beautiful, today, breakfast]\n",
       "quick            [breakfast, today, jumps]\n",
       "breakfast         [toast, sausages, bacon]\n",
       "blue                   [green, brown, sky]\n",
       "dog             [fox, breakfast, sausages]\n",
       "jumps                    [sky, quick, dog]\n",
       "kings         [sausages, beautiful, today]\n",
       "beautiful              [love, green, blue]\n",
       "beans              [sausages, bacon, eggs]\n",
       "green             [blue, brown, beautiful]\n",
       "today         [breakfast, beautiful, love]\n",
       "fox                     [dog, blue, brown]\n",
       "eggs              [sausages, bacon, beans]\n",
       "sky               [blue, green, beautiful]\n",
       "bacon               [ham, sausages, toast]\n",
       "lazy                     [quick, fox, dog]\n",
       "sausages               [bacon, ham, beans]\n",
       "toast              [bacon, ham, breakfast]\n",
       "ham               [bacon, sausages, toast]\n",
       "brown                 [green, blue, beans]\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(unique_words)\n",
    "similarity_df.apply(lambda row: feature_names[np.argsort(-row.values)[1:4]], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c257a",
   "metadata": {},
   "source": [
    "# The FastText Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52044570",
   "metadata": {},
   "source": [
    "The FastText model was first introduced by Facebook in 2016 as an extension and supposedly improvement of the vanilla Word2Vec model. Based on the original paper titled ‘Enriching Word Vectors with Subword Information’ by Mikolov et al. which is an excellent read to gain an in-depth understanding of how this model works. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on GitHub and claims to have the following.\n",
    "\n",
    "- Recent state-of-the-art English word vectors.\n",
    "- Word vectors for 157 languages trained on Wikipedia and Crawl.\n",
    "- Models for language identification and various supervised tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db0044",
   "metadata": {},
   "source": [
    "Though I haven't implemented this model from scratch, based on the research paper, following is what I learnt about how the model works. In general, predictive models like the Word2Vec model typically considers each word as a distinct entity (e.g. where) and generates a dense embedding for the word. However this poses to be a serious limitation with languages having massive vocabularies and many rare words which may not occur a lot in different corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddffd3",
   "metadata": {},
   "source": [
    "The Word2Vec model typically ignores the morphological structure of each word and considers a word as a single entity. The FastText model **considers each word as a Bag of Character n-grams**. This is also called as a **subword model** in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08d653",
   "metadata": {},
   "source": [
    "We add special boundary symbols **<** and **>** at the beginning and end of words. This enables us to distinguish prefixes and suffixes from other character sequences. We also include the word **w** itself in the set of its n-grams, to learn a representation for each word (in addition to its character n-grams)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba806617",
   "metadata": {},
   "source": [
    "Taking the word where and **n=3 (tri-grams)** as an example, it will be represented by the **character n-grams: <wh, whe, her, ere, re>** and the special sequence **< where >** representing the whole word. Note that the sequence , corresponding to the word **< her >** is different from the tri-gram **her** from the word **where**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30944d4b",
   "metadata": {},
   "source": [
    "In practice, the paper recommends in extracting all the n-grams for **n ≥ 3** and **n ≤ 6**. This is a very simple approach, and different sets of n-grams could be considered, for example taking all prefixes and suffixes. We typically associate a vector representation (embedding) to each n-gram for a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6273c81",
   "metadata": {},
   "source": [
    "Thus, we can represent a word by the sum of the vector representations of its n-grams or the average of the embedding of these n-grams. Thus, due to this effect of leveraging n-grams from individual words based on their characters, there is a higher chance for rare words to get a good representation since their character based n-grams should occur across other words of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51de035",
   "metadata": {},
   "source": [
    "# Robust FastText Model with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b113b",
   "metadata": {},
   "source": [
    "The **gensim** package has nice wrappers providing us interfaces to leverage the FastText model available under the gensim.models.fasttext module. Let’s apply this once again on our toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokenized_corpus, vector_size=feature_size, \n",
    "                              window=window_context, min_count = min_word_count,\n",
    "                              sg=sg, sample=sample, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af62135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "954dab23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x7f859e7d7bb0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "ft_model = FastText(tokenized_corpus, vector_size=feature_size, \n",
    "                     window=window_context, min_count = min_word_count,\n",
    "                     sg=sg, sample=sample, epochs=5000)\n",
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "deb73d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = ft_model.wv.index_to_key\n",
    "wvs = ft_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='green', edgecolors='k')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2da45",
   "metadata": {},
   "source": [
    "## Embedding Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69c58db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.8354679 ,  0.24270937,  1.5270482 ,  0.29889134, -0.20860039,\n",
       "        -0.17280957,  0.2009739 ,  0.71061313,  0.8451077 ,  0.23712301,\n",
       "        -0.0509496 , -0.50620085,  0.0536426 , -0.8318011 ,  0.30094525],\n",
       "       dtype=float32),\n",
       " (15,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv['sky'], ft_model.wv['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d51d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3631966\n",
      "0.95235574\n"
     ]
    }
   ],
   "source": [
    "print(ft_model.wv.similarity(w1='ham', w2='sky'))\n",
    "print(ft_model.wv.similarity(w1='ham', w2='sausages'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4cae9d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd one out for [ dog fox ham ]: ham\n",
      "Odd one out for [ bacon ham sky sausages ]: sky\n"
     ]
    }
   ],
   "source": [
    "st1 = \"dog fox ham\"\n",
    "print('Odd one out for [',st1, ']:',  \n",
    "      ft_model.wv.doesnt_match(st1.split()))\n",
    "\n",
    "st2 = \"bacon ham sky sausages\"\n",
    "print('Odd one out for [',st2, ']:', \n",
    "      ft_model.wv.doesnt_match(st2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b982e",
   "metadata": {},
   "source": [
    "### Getting document level embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c77b19",
   "metadata": {},
   "source": [
    "Now suppose we wanted to cluster the eight documents from our toy corpus, we would need to get the document level embeddings from each of the words present in each document. One strategy would be to average out the word embeddings for each word in a document. This is an extremely useful strategy and you can adopt the same for your own problems. Let’s apply this now on our corpus to get features for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "947fd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41febcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.633224</td>\n",
       "      <td>0.241109</td>\n",
       "      <td>1.336286</td>\n",
       "      <td>0.227798</td>\n",
       "      <td>-0.196780</td>\n",
       "      <td>-0.039579</td>\n",
       "      <td>0.184827</td>\n",
       "      <td>0.730348</td>\n",
       "      <td>0.824477</td>\n",
       "      <td>0.126218</td>\n",
       "      <td>-0.025842</td>\n",
       "      <td>-0.348504</td>\n",
       "      <td>0.098324</td>\n",
       "      <td>-0.786467</td>\n",
       "      <td>0.234595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.558180</td>\n",
       "      <td>0.151005</td>\n",
       "      <td>1.171184</td>\n",
       "      <td>0.287862</td>\n",
       "      <td>-0.219503</td>\n",
       "      <td>-0.049056</td>\n",
       "      <td>0.193210</td>\n",
       "      <td>0.688771</td>\n",
       "      <td>0.583352</td>\n",
       "      <td>0.177741</td>\n",
       "      <td>-0.110026</td>\n",
       "      <td>-0.388595</td>\n",
       "      <td>0.054427</td>\n",
       "      <td>-0.672323</td>\n",
       "      <td>0.224093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.714791</td>\n",
       "      <td>-0.460922</td>\n",
       "      <td>1.369773</td>\n",
       "      <td>-0.143889</td>\n",
       "      <td>0.404819</td>\n",
       "      <td>0.906332</td>\n",
       "      <td>0.107537</td>\n",
       "      <td>0.466878</td>\n",
       "      <td>0.489146</td>\n",
       "      <td>-0.690582</td>\n",
       "      <td>0.451690</td>\n",
       "      <td>0.332470</td>\n",
       "      <td>-0.088589</td>\n",
       "      <td>-0.898453</td>\n",
       "      <td>-0.215381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.369935</td>\n",
       "      <td>-0.693844</td>\n",
       "      <td>0.548159</td>\n",
       "      <td>0.798238</td>\n",
       "      <td>-0.120878</td>\n",
       "      <td>0.261229</td>\n",
       "      <td>0.257672</td>\n",
       "      <td>1.025583</td>\n",
       "      <td>-0.979814</td>\n",
       "      <td>0.572465</td>\n",
       "      <td>-0.527470</td>\n",
       "      <td>-0.788771</td>\n",
       "      <td>-0.573670</td>\n",
       "      <td>-0.596745</td>\n",
       "      <td>0.486590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.005354</td>\n",
       "      <td>-0.499769</td>\n",
       "      <td>0.598558</td>\n",
       "      <td>0.644029</td>\n",
       "      <td>-0.199926</td>\n",
       "      <td>0.119552</td>\n",
       "      <td>0.221546</td>\n",
       "      <td>0.770649</td>\n",
       "      <td>-0.690052</td>\n",
       "      <td>0.469918</td>\n",
       "      <td>-0.544808</td>\n",
       "      <td>-0.664936</td>\n",
       "      <td>-0.435627</td>\n",
       "      <td>-0.498187</td>\n",
       "      <td>0.462443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.524913</td>\n",
       "      <td>-0.349459</td>\n",
       "      <td>1.343731</td>\n",
       "      <td>-0.115450</td>\n",
       "      <td>0.314273</td>\n",
       "      <td>0.762364</td>\n",
       "      <td>0.118351</td>\n",
       "      <td>0.495899</td>\n",
       "      <td>0.555722</td>\n",
       "      <td>-0.595158</td>\n",
       "      <td>0.390288</td>\n",
       "      <td>0.242537</td>\n",
       "      <td>-0.047782</td>\n",
       "      <td>-0.861928</td>\n",
       "      <td>-0.161288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.675773</td>\n",
       "      <td>0.260902</td>\n",
       "      <td>1.350646</td>\n",
       "      <td>0.276515</td>\n",
       "      <td>-0.224001</td>\n",
       "      <td>-0.084514</td>\n",
       "      <td>0.198552</td>\n",
       "      <td>0.759499</td>\n",
       "      <td>0.851364</td>\n",
       "      <td>0.165621</td>\n",
       "      <td>-0.023771</td>\n",
       "      <td>-0.400847</td>\n",
       "      <td>0.123324</td>\n",
       "      <td>-0.789908</td>\n",
       "      <td>0.229319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.698170</td>\n",
       "      <td>-0.448786</td>\n",
       "      <td>1.392484</td>\n",
       "      <td>-0.131797</td>\n",
       "      <td>0.401764</td>\n",
       "      <td>0.901194</td>\n",
       "      <td>0.105864</td>\n",
       "      <td>0.462250</td>\n",
       "      <td>0.512573</td>\n",
       "      <td>-0.685103</td>\n",
       "      <td>0.454156</td>\n",
       "      <td>0.336223</td>\n",
       "      <td>-0.081693</td>\n",
       "      <td>-0.904762</td>\n",
       "      <td>-0.211872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.633224  0.241109  1.336286  0.227798 -0.196780 -0.039579  0.184827   \n",
       "1  0.558180  0.151005  1.171184  0.287862 -0.219503 -0.049056  0.193210   \n",
       "2 -0.714791 -0.460922  1.369773 -0.143889  0.404819  0.906332  0.107537   \n",
       "3 -0.369935 -0.693844  0.548159  0.798238 -0.120878  0.261229  0.257672   \n",
       "4 -0.005354 -0.499769  0.598558  0.644029 -0.199926  0.119552  0.221546   \n",
       "5 -0.524913 -0.349459  1.343731 -0.115450  0.314273  0.762364  0.118351   \n",
       "6  0.675773  0.260902  1.350646  0.276515 -0.224001 -0.084514  0.198552   \n",
       "7 -0.698170 -0.448786  1.392484 -0.131797  0.401764  0.901194  0.105864   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.730348  0.824477  0.126218 -0.025842 -0.348504  0.098324 -0.786467   \n",
       "1  0.688771  0.583352  0.177741 -0.110026 -0.388595  0.054427 -0.672323   \n",
       "2  0.466878  0.489146 -0.690582  0.451690  0.332470 -0.088589 -0.898453   \n",
       "3  1.025583 -0.979814  0.572465 -0.527470 -0.788771 -0.573670 -0.596745   \n",
       "4  0.770649 -0.690052  0.469918 -0.544808 -0.664936 -0.435627 -0.498187   \n",
       "5  0.495899  0.555722 -0.595158  0.390288  0.242537 -0.047782 -0.861928   \n",
       "6  0.759499  0.851364  0.165621 -0.023771 -0.400847  0.123324 -0.789908   \n",
       "7  0.462250  0.512573 -0.685103  0.454156  0.336223 -0.081693 -0.904762   \n",
       "\n",
       "         14  \n",
       "0  0.234595  \n",
       "1  0.224093  \n",
       "2 -0.215381  \n",
       "3  0.486590  \n",
       "4  0.462443  \n",
       "5 -0.161288  \n",
       "6  0.229319  \n",
       "7 -0.211872  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get document level embeddings\n",
    "ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n",
    "                                             num_features=feature_size)\n",
    "pd.DataFrame(ft_doc_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a2eb6",
   "metadata": {},
   "source": [
    "### Trying out document clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edd223",
   "metadata": {},
   "source": [
    "Now that we have our features for each document, let’s cluster these documents using the Affinity Propagation algorithm, which is a clustering algorithm based on the concept of “message passing” between data points and does not need the number of clusters as an explicit input which is often required by partition-based clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8bf03c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document  \\\n",
       "0                                      The sky is blue and beautiful.   \n",
       "1                                   Love this blue and beautiful sky!   \n",
       "2                        The quick brown fox jumps over the lazy dog.   \n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans   \n",
       "4                         I love green eggs, ham, sausages and bacon!   \n",
       "5                    The brown fox is quick and the blue dog is lazy!   \n",
       "6            The sky is very blue and the sky is very beautiful today   \n",
       "7                         The dog is lazy but the brown fox is quick!   \n",
       "\n",
       "  Category  ClusterLabel  \n",
       "0  weather             0  \n",
       "1  weather             0  \n",
       "2  animals             2  \n",
       "3     food             1  \n",
       "4     food             1  \n",
       "5  animals             2  \n",
       "6  weather             0  \n",
       "7  animals             2  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "ap = AffinityPropagation()\n",
    "ap.fit(ft_doc_features)\n",
    "\n",
    "cluster_labels = ap.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, \n",
    "                              columns=['ClusterLabel'])\n",
    "\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a2264",
   "metadata": {},
   "source": [
    "We can see that our algorithm has clustered each document into the right group based on our Word2Vec features. Pretty neat! We can also visualize how each document in positioned in each cluster by using Principal Component Analysis (PCA) to reduce the feature dimensions to 2-D and then visualizing the same (by color coding each cluster).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00d63549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pcs = pca.fit_transform(ft_doc_features)\n",
    "labels = ap.labels_\n",
    "categories = list(corpus_df['Category'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    color = 'orange' if label == 0 else 'blue' if label == 1 else 'green'\n",
    "    annotation_label = categories[i]\n",
    "    x, y = pcs[i]\n",
    "    plt.scatter(x, y, c=color, edgecolors='k')\n",
    "    plt.annotate(annotation_label, xy=(x+1e-2, y+1e-2), xytext=(0, 0), \n",
    "                 textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8f92b",
   "metadata": {},
   "source": [
    "Everything looks to be in order as documents in each cluster are closer to each other and far apart from other clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
