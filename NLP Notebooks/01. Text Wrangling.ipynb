{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bfc76e",
   "metadata": {},
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1c83a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/keiziapurba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/keiziapurba/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/keiziapurba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/keiziapurba/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245328e5",
   "metadata": {},
   "source": [
    "# Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc80ded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over The Big Dog'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The quick brown fox jumped over The Big Dog'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c190650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumped over the big dog'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512576a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE QUICK BROWN FOX JUMPED OVER THE BIG DOG'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b272ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Quick Brown Fox Jumped Over The Big Dog'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b7eb8",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f6ece5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d74ecee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
       " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
       " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
       " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aabf87e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f7b706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: jinja2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52ccd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.4.0) (3.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: jinja2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: setuptools in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "download(model=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df3d2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text_spacy = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d11f1230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
       " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
       " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
       " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obj.text for obj in text_spacy.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2c69af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
     ]
    }
   ],
   "source": [
    "print([obj.text for obj in text_spacy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114f55c",
   "metadata": {},
   "source": [
    "# Removing HTML tags & noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa95c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* */\r\n",
      "hr {\r\n",
      "    width: 45%;\r\n",
      "    /* adjust to ape original work */\r\n",
      "    margin-top: 1em;\r\n",
      "    /* space above & below */\r\n",
      "    margin-bottom: 1em;\r\n",
      "    margin-left: auto;\r\n",
      "    /* these two ensure a.. */\r\n",
      "    margin-right: auto;\r\n",
      "    /* ..centered rule */\r\n",
      "    clear: both;\r\n",
      "    /* don't let sidebars & floats overlap rule */\r\n",
      "    }\r\n",
      "/* ************************************************************************\r\n",
      " * Images and captions\r\n",
      " * ********************************************************************** */\r\n",
      "img {\r\n",
      "    /* the default inline image has */\r\n",
      "    border: 1px solid black;\r\n",
      "    /* a thin black line border.. */\r\n",
      "    padding: 6px;\r\n",
      "    /* ..spaced a bit out from the graphic */\r\n",
      "    }</style>\r\n",
      "<link rel=\"schema.dc\" href=\"http://purl.org/dc/elements/1.1/\">\r\n",
      "<link rel=\"schema.dcterms\" href=\"http://purl.org/dc/terms/\">\r\n",
      "<meta name=\"dc.title\" content=\"The Bible, King James version, Book 1: Genesis\">\r\n",
      "<meta name=\"dc.language\" content=\"en\">\r\n",
      "<meta name=\"dcterms.source\" content=\"https://www.gutenberg.org/files/8001/8001.txt\">\r\n",
      "<meta name=\"dcterms.modified\" content=\"2022-07-04T11:14:47.844306+00:00\">\r\n",
      "<meta name=\"dc.rights\" content=\"Public domain in the USA.\">\r\n",
      "<lin\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "content = data.text\n",
    "print(content[2745:3948])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a63205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d of schedule]\n",
      "[This file was first posted on June 7, 2003]\n",
      "Edition: 10\n",
      "Language: English\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\n",
      "This eBook was produced by David Widger\n",
      "with the help of Derek Andrew's text from January 1992\n",
      "and the work of Bryan Taylor in November 2002.\n",
      "Book 01        Genesis\n",
      "01:001:001 In the beginning God created the heaven and the earth.\n",
      "01:001:002 And the earth was without form, and void; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "01:001:003 And God said, Let there be light: and there was light.\n",
      "01:001:004 And God saw the light, that it was good: and God divided the\n",
      "Â Â Â Â Â Â Â Â Â Â Â light from the darkness.\n",
      "01:001:005 And God called the light Day, and the\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[1163:1957])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1a61aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f59bf9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SÃ³mÄ› ÃccÄ›ntÄ›d tÄ›xt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'SÃ³mÄ› ÃccÄ›ntÄ›d tÄ›xt'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4020733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_accented_chars(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23179b",
   "metadata": {},
   "source": [
    "# Removing Special Characters, Numbers and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7616aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f0204b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ ðŸ™‚ðŸ™‚ðŸ™‚'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ ðŸ™‚ðŸ™‚ðŸ™‚\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d653d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun See you at  What do you think  '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(s, remove_digits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e7614ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun See you at 730 What do you think 9318 '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bcbb4",
   "metadata": {},
   "source": [
    "# Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b555977f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: anyascii in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: textsearch in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from textsearch) (1.4.4)\n",
      "Requirement already satisfied: anyascii in /Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages (from textsearch) (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ec21475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb025945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I'm\", 'I am'),\n",
       " (\"I'm'a\", 'I am about to'),\n",
       " (\"I'm'o\", 'I am going to'),\n",
       " (\"I've\", 'I have'),\n",
       " (\"I'll\", 'I will'),\n",
       " (\"I'll've\", 'I will have'),\n",
       " (\"I'd\", 'I would'),\n",
       " (\"I'd've\", 'I would have'),\n",
       " ('Whatcha', 'What are you'),\n",
       " (\"amn't\", 'am not')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "list(contractions.contractions_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30a6bfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all cannot expand contractions I would think! You would not be able to. How did you do it?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447d7ee",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a09b4243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "877ea6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lie'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b18e65af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strang'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990203b",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60b5c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/keiziapurba/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b61ac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmatize in module nltk.stem.wordnet:\n",
      "\n",
      "lemmatize(word: str, pos: str = 'n') -> str method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
      "    Lemmatize `word` using WordNet's built-in morphy function.\n",
      "    Returns the input word unchanged if it cannot be found in WordNet.\n",
      "    \n",
      "    :param word: The input word to lemmatize.\n",
      "    :type word: str\n",
      "    :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
      "        `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
      "        for satellite adjectives.\n",
      "    :param pos: str\n",
      "    :return: The lemma of `word`, for the given `pos`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wnl.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43b1da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "box\n"
     ]
    }
   ],
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('boxes', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6bff936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "137d9d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e0985c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))\n",
    "print(wnl.lemmatize('fancier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccb61160",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c3241",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd85a50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(s)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac66a9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox are quick and they are jumping over the sleeping lazy dog !'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_text = ' '.join(wnl.lemmatize(token) for token in tokens)\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470be7c1",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a31f6103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28f906",
   "metadata": {},
   "source": [
    "# Tag conversion to WordNet Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e833e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49df7d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'n'), ('brown', 'a'), ('foxes', 'n'), ('are', 'v'), ('quick', 'a'), ('and', 'n'), ('they', 'n'), ('are', 'v'), ('jumping', 'v'), ('over', 'n'), ('the', 'n'), ('sleeping', 'v'), ('lazy', 'a'), ('dogs', 'n'), ('!', 'n')]\n"
     ]
    }
   ],
   "source": [
    "wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "print(wordnet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fb5a6",
   "metadata": {},
   "source": [
    "# Effective Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc5ce6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a707732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aff4319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def wordnet_lemmatize_text(text):\n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1ebf521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3efcab46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatize_text(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c74c1a",
   "metadata": {},
   "source": [
    "# Lemmatization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6243b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'entity'])\n",
    "\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b275e9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c300d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiziapurba/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the brown foxes are quick and they are jumping over the sleeping lazy dogs !'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_lemmatize_text(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47283ca3",
   "metadata": {},
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bab80c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "91591ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "005b7eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0911565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brown foxes quick jumping sleeping lazy dogs !'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(s, is_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aced8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2bb6617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove('the')\n",
    "stop_words.append('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7442da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The foxes quick jumping the sleeping lazy dogs !'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(s, is_lower_case=False, stopwords=stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
